# 2. Solution to Nonlinear Equations

## 2.1. Bisection

### Problem

- given a univariate and continuous function $g(x)$ , we are interested in a value $x_0$ such that $g(x_0) = 0$. $x_0$ is called **zero point** of $g$. Note no derivatives of g(x) are required.

### Theorem: *Intermediate Value Theorem*

- If $f(a) · f(b) < 0 (a < b)$, due to the continuity of function $f$, there exists at least one zero point between $a$ and $b$.

#### Algorithm 

​	**INPUT**: continuous and univariate function f and interval $[a, b]$ with $f (a)f (b) < 0$.

​	**INITIALIZE**: $a^{(0)}$ ← $a$ and $b^{(0)}$ ← $b$, and $t$ ← 0.

​	**Repeat**

​		calculate $c^{(t)} ← \frac{a(t)+b(t)}{2}$;

​		If $f(c^{(t)}) · f(a^{(t)}) < 0$, let $a^{(t+1)} ← a^{(t)}$ and $b^{(t+1)} ← c^{(t)}$; 

​			else if $f(c^{(t)}) · f(b^{(t)}) < 0$, let $a^{(t+1)} ← c^{(t)}$ and $b^{(t+1)} ← b^{(t)}$;  

​			else break;

​		$t \leftarrow t+1$

​	**Until** $|a^{(t)} - b^{(t)}| < \epsilon$ 

​	**OUTPUT**: $a^{(t)} , b^{(t)}$  in the last iteration. $c^{(t)} ← \frac{a(t)+b(t)}{2}$ is the final answer.

- Note: $\frac{b-a}{2^n} < \epsilon \rightarrow \ n>log_{2}\frac{b-a}{\epsilon}$ 

### Find the <u>shortest confidence interval</u> 

### <img src="/Users/rosa/Documents/github_repository/Gavin-OP.github.io/public/notes/stat/Statistical Computing/image/2.1.png" alt="2.1" style="zoom:50%;" />

- Question:  $Y$ is a random variable with known probability density function $f(y)$. Given $α_0 (e.g. α_0 = 0.95)$, we want to find the shortest interval [a, b] s.t. $P (a ≤ Y ≤ b) = α_0$ or equivalently $\int^b_a f(y)dy = α_0$. 
- Proposition: assume $f$ is unimodal. If one interval $[a^∗,b^∗]$ satisfies $\int^{b^*}_{a^*} f(y)dy = α_0$ and $f(a^∗) = f(b^∗)$, then for any other interval [a,b] with $\int^b_a f(y)dy = α_0$, we have $b^∗ −a^∗ < b−a$.
- Solution: based on the proposition, the shortest interval $[a, b]$ must satisfy $f (a) = f (b)$. We let $λ$ be the value of f(a) and f(b). For any fixed λ, we use the bisection method to find $a_λ$ and $b_λ$, respectively, s.t. $f(a_λ) ≈ λ, f(b_λ) ≈ λ$ and $a_λ < b_λ$ (inside loop). Then we numerically calculate the value of $\int^{b_λ}_{a_λ} f(y)dy = α_0, α(λ)$. Finally we apply the bisection method to finding the $a_λ$ zero point of $α(λ) − α_0$(outside loop)

#### **Algorithm** 

​	**INPUT**: continuous, univariate and unimodal function f, $α_0$(e.g. 0.95), tolerance $ε$(e.g. e-5), a small value $λ_{lw}$ (near zero), and a large value $λ_{up}$ (near the maximum of f).

​	**INITIALIZE**: $λ_{lw}^{(0)}$ ← $λ_{lw}$ and $λ_{up}^{(0)}$ ← $λ_{up}$, and $t$ ← 0.

​	**Repeat**

​		calculate $λ_{mid}^{(t)} ← \frac{λ_{up}^{(t)}+λ_{lw}^{(t)}}{2}$;

​		Use the bisection method to find one zero point of $f (x)-λ_{mid}^{(t)} = 0, \tilde{a}$;

​		Use the bisection method again to find another zero point of $f (x)-λ_{mid}^{(t)} = 0, \tilde{b}$;

​		(Without loss of generality, we assume $\tilde{a} < \tilde{b}$.)

​		Numerically integrate f(x) from $\tilde{a}$ to $\tilde{b}$, the result is denoted by $α(λ_{mid}^{(t)})$;

​		else if $α(λ_{mid}^{(t)}) > α_0$, let $λ_{lw}^{(t+1)} \leftarrow λ_{mid}^{(t)}$ and $λ_{up}^{(t+1)} \leftarrow λ_{up}^{(t)}$

​		else break;

​		$t \leftarrow t+1$

​	**Until** $|a^{(t)} - b^{(t)}| < \frac{\epsilon}{2}$ 

​	**OUTPUT**: $[\tilde{a}  ,\tilde{b}]$  in the last iteration with confidence level $\alpha _0$ 

## 2.2. Functional Iteration

- When we search **a maximum (or minimum) for a differentiable function h(x)**, we usually turn to solving the equation $dh(x) dx = 0$, i.e.

$$
\frac{dh(x)}{dx}+x=x
$$

​	Let $f(x)$ be $\frac{dh(x)}{dx} +x$, the equation (1) becomes
$$
f(x) = x
$$
​	All $x^∗$ solving equation (2) ($f(x^∗ ) = x^∗$ ) are called the ***fixed points*** of f(x). Generally, our problem is that, for a function $f$ which may be <u>non-differentiable</u>, we would like to **find a fixed point of $f(x)$** .

### **Algorithm**: *Fixed point finding algorithm*

- **Input**: continuous and univariate function $f$, maximum number of iterations $T$, and tolerance $ϵ$ ; initial point $x(0)$ .

  **Output**: x(t) in the last iteration.

  1: t ← 0 

  2: **repeat** 

  3: let $y$ be $x^{(t)}$ ; 

  4: calculate $x^{(t+1)}=f(y)$; 

  5: t ← t + 1; 

  6: until $|x^{(t)}−y|<ϵ$ or $t≥T$.

  > do iteration until the solutions are always "around" a number

- e.g.  Given a positive number a, find $\sqrt{a}$.

  Sol: notice that a is the solution of the equation $\frac{1}{2}(\frac{a}{x}−x)=0$. Let $f(x)=\frac{1}{2}(\frac{a}{x}−x)+x=\frac{1}{2}(\frac{a}{x}+x)$, we implement the algorithm above by $x^{(t+1)}=\frac{1}{2}(\frac{a}{x^{}(t)}+x^{(t)})$. 

  Q: Why don’t we take $\tilde f(x)=(\frac{a}{x})+x=\frac{a}{x}$? We have the following proposition.

### ***Proposition**

Suppose $f: I \rightarrow \mathbb{R}$, where $I$ is a closed interval such that

​	(1) $f(x) \in I \ for \ \forall x$

​	(2) $|f(y) − f(x)| ≤ λ|y − x|$ (Lipschitz continuous) for a constant $λ$ (Lipschitz constant) and $∀x, y ∈ I$.

If $λ ∈ [0, 1)$, then

​	(1) $f(x)$ has a **unique fixed point** $x_∞ ∈ I$.

​	(2) the sequence $x_n = f(x_{n−1})$ goes to $x_∞, ∀x_0 ∈ I$ 

​	(3) $|x_n − x_∞| ≤ \frac{λ^n}{1-λ}|x_1 − x_0|$ 

​	Proof: 	

​		$|x_{k+1} − x_k| = |f(x_k) − f(x_{k−1)}|$

​				$ \leq λ|x_k − x_{k−1}| ≤ λ^2 |x_{k−1} − x_{k−2}| ≤ . . . ≤ λ^k |x_1 − x_0|$

​		$∀m > n, |x_m − x_n| ≤ \sum_{K=n}^{m-1} |x_{k+1} − x_k| ≤ \sum_{K=n}^{m-1} λ^k |x_1 − x_0| ≤ \frac{λ^n}{1-λ}|x_1 − x_0|$

​	The last inequality indicates that {xn}n=1∞ is a Cauchy sequence. 

> <img src="/Users/rosa/Documents/github_repository/Gavin-OP.github.io/public/notes/stat/Statistical Computing/image/2.2.png" alt="2.2" style="zoom:50%;" /><img src="/Users/rosa/Documents/github_repository/Gavin-OP.github.io/public/notes/stat/Statistical Computing/image/2.3.png" alt="2.3" style="zoom:50%;" />

​	In $\mathbb{R}$, Cauchy sequence implies the convergence of the sequence, so ${\{x_n\}}^∞_{n=1}$ converges to a point $x_∞$. Moreover, ${\{x_n\}}^∞_{n=1} ∈ I$ and $I$ is closed, so $x_∞ ∈ I$. *(2) is proved*.

​	For the equation $x_n = f(x_{n−1})$, let $n$ go to infinity and notice $f$ is continuous, so we have $x_∞ = f(x_∞)$. If there exists a $y \neq x_∞ $ s.t. $y = f(y)$, then

​								$$ |y − x_∞| = |f(y) − f(x_∞)|≤ λ|y − x_∞|
< |y − x_∞|.$$

​	The last inequality holds, because λ∈[0,1). It is contradictory that |y−x∞|<|y−x∞|, so x∞ is the unique fixed point of f. We *proved (1)*. (3) can be easily proved, so we omit it.

- e.g. $\tilde{f}(x) = \frac{a}{x}, x>0$ 

  ​						$$ |\tilde{f}(y) - \tilde{f}(x)| = |\frac{a}{y}-\frac{a}{x}|=\frac{a(x-y)}{xy}|=|\frac{a}{xy}||y-x|=\frac{a}{xy}|y-x|$$

  We need to find $I=[c,d]$ such that

  ​	(1) $\tilde{f}(x) \in [c,d], \forall x \in [c,d]$;

  ​	(2) $\frac{a}{xy} < 1, \forall x \in [c,d]$.

  (2) implies that $\frac{a}{c^2} <1$, so $c>\sqrt{a}, \ \sqrt{a} \notin I = [c,d]$. Therefore, we do not use $\tilde{f}(x)$ as the iteration operator to find $\sqrt{a}$.

  ​	As to $f(x) = \frac{1}{2} ( \frac{a}{x} + x), x > 0$,

  ​	

  ​										$$ |f(y) - f(x)| = |\frac{1}{2}(\frac{a}{y}+y)-\frac{1}{2}(\frac{a}{x}+x)|\\ =\frac{1}{2}|\frac{a}{y}-\frac{a}{x} + (y-x)|\\ =\frac{1}{2}|\frac{a}{xy}(x-y)+(y-x)|\\=\frac{1}{2}|1-\frac{a}{xy}||y-x|$$

  ​	Consider the interval $I = [ \sqrt{\frac{2a}{3}} ,\sqrt{2a}],\sqrt{a} \in I$. For $\forall x \in I$, $f(x) \in I$. Additionally, for $\forall x,y \in I, |1- \frac{a}{xy}| \leq \frac{1}{2}$, so $f(x)$ is Lipschitz continuous on $I$. Therefore, we can use the iterated operation $x_n = \frac{1}{2}(\frac{a}{x_{n-1}}+x_{n-1})$ to aprroximate $\sqrt{a}$. 

  As to $f(x) = \frac{1}{2} ( \frac{a}{x} + x), x > 0$,

  ​	

  ​										$$ |f(y) - f(x)| = |\frac{1}{2}(\frac{a}{y}+y)-\frac{1}{2}(\frac{a}{x}+x)|\\ =\frac{1}{2}|\frac{a}{y}-\frac{a}{x} + (y-x)|\\ =\frac{1}{2}|\frac{a}{xy}(x-y)+(y-x)|\\=\frac{1}{2}|1-\frac{a}{xy}||y-x|$$

  ​	Consider the interval $I = [ \sqrt{\frac{2a}{3}} ,\sqrt{2a}],\sqrt{a} \in I$. For $\forall x \in I$, $f(x) \in I$. Additionally, for $\forall x,y \in I, |1- \frac{a}{xy}| \leq \frac{1}{2}$, so $f(x)$ is Lipschitz continuous on $I$. Therefore, we can use the iterated operation $x_n = \frac{1}{2}(\frac{a}{x_{n-1}}+x_{n-1})$ to aprroximate $\sqrt{a}$. 

  ​	For illustration, when $a = 2 $ and $x_0 = 1.7$,

  ​	

  ​												$$ x_1=\tilde{f}(x_0)=\frac{2}{1.7}=1.176471\\x_2=\tilde{f}(x_1)=x_0=1.7\\x_3=x_1\\x_4=x_2=x_0$$

  ​	In contract,

  ​        								$$x_1=f(x_0)=\frac{1}{2}(\frac{2}{1.7}+1.7)=1.438235\\x_2=f(x_1)=\frac{1}{2}(\frac{2}{1.438235}+1.438235)=1.414414\\x_3=1.414214...$$

  ​	After three iterations, the result is very close to $\sqrt{2}$.

- Q: How to verify $f$ satisfies the two requirements of the proposition?

> (1) $f(x) \in I \ for \ \forall x$ 
>
> (2) $|f(y) − f(x)| ≤ λ|y − x|$ (Lipschitz continuous) for a constant $λ$ (Lipschitz constant) and $∀x, y ∈ I$.

### Lagrange’s Mean Value Theorem

- if $f$ is continuous on the closed interval $[a, b]$ and differentiable on the open interval $(a, b)$, then there exists a point $ξ$ in $(a, b)$ such that

​										    	$$f^{′}(ξ)=\frac{f(b)-f(a)}{b-a}.$$

Solution(a sufficient condition): first, we have to find an interval $[a, b]$ s.t. $f$ is <u>continuous</u> on $[a, b]$ and <u>differentiable</u> on$ (a, b)$, and $f(x) ∈ [a, b]$ when $x ∈ [a, b]$. Second, by the mean value theorem, if there <u>exist a constant $λ$</u> s.t.  $1 > λ ≥ \sup_\limits{ξ\in (a,b)} |f^{'}(ξ)|$ , then $|f(x) − f(y)| ≤ λ|x − y|$. 

When the two conditions hold, the corresponding $f$ satisfies the two requirements of the proposition.

## 2.3. Newton's Method

- Purpose: **find the maximum (or minimum) for a function $f$**. 
- Assume function $f(x)$ is twice differentiable. Let $g(x)$ be $f_0 (x)$. In most cases, finding optimum of $f(x)$ is equivalent to finding the solution of the equation $g(x) = 0$. 
- Two perspectives that motivates the Newton method:
  1. Considering the equation $g(x) = 0$, from a starting point $x^{ (0)}$, we draw a line that is tangent to $g(x)$ at point $(x_0, g(x_0))$ . We can regard this line as an locally approximate curve to $g(x)$. After some simple algebra, this line $l_0(x)$ has the expression $l_0(x) = g(x_0) + g^{'}(x_0)(x − x_0)$. As $l_0(x)$ is approximate to g(x), the solution of $l_0(x) = 0$ is probably close to the solution of $g(x) = 0$. By solving $l_0(x) = 0$, we get the solution $x_1 = x_0 − \frac{g(x_0)}{ g^{'}(x_0)}$ . Repeat the procedure, and then we have the general step $x_n = x_{n−1} −\frac{ g(x_n−1)}{ g^{'}(x_n−1)}$ to <u>find the solution of $g(x) = 0$.</u> <img src="/Users/rosa/Documents/github_repository/Gavin-OP.github.io/public/notes/stat/Statistical Computing/image/2.4.png" alt="2.4" style="zoom:50%;" />

2. Notice that when we **minimize (or maximize) a convex function $f(x)$,** the problem is equivalent to finding the solution $g(x) = f^{'}(x) = 0$. Plug $f^{'}(x)$ into $x_n = x_{n−1} − \frac {g(x_{n−1})}{g^{'}(x_{n−1})}$ , we have $x_n = x_{n−1} − \frac {f^{'}(x_{n−1})}{f^{''}(x_{n−1})}$.   $\rightarrow$   When we minimize $f(x)$, given a starting point $x_0$, the <u>Taylor expansion</u> of $f(x)$ at $x_0$ (omit cubic term and terms with higher order) is $q_0(x) = f(x_0) + f^{'}(x_0)(x − x_0) + \frac{f ^{''}(x_0)} {2} (x − x_0)^2$ . $q_0(x)$ can be regarded as an <u>locally approximate curve</u> to the function $f(x)$. Therefore, the point that minimizes $q_0(x)$ is probably close to the point that minimized $f(x)$. By minimizing $q_0(x)$, we get the point $x_1 = x_0 − \frac {f^{'}(x_{0})}{f^{''}(x_{0})}$ . Repeat the procedure multiple times, we have the general step: $x_n = x_{n−1} − \frac {f^{'}(x_{n−1})}{f^{''}(x_{n−1})}$. <img src="/Users/rosa/Documents/github_repository/Gavin-OP.github.io/public/notes/stat/Statistical Computing/image/2.5.png" alt="2.5" style="zoom:50%;" />

## 2.4. Rate of Covergence

- Definition: Assume ${\{x_n\}}^∞_{n=0} \rightarrow x^*$. If $\exist\  p \geq 1$ and $α > 0 $ s.t. $\lim\limits_{n \rightarrow ∞} \frac{||x_{n+1}−x_∞||}{||x_{n}−x_∞||^p}=\alpha$, then  ${\{x_n\}}^∞_{n=0} $ is <u>p-order convergence</u>.

  > The limit describes the relationship between the distance of $x_{n+1}$ to $x_{\infty}$ and the distance of $x_{n}$ to $x_{\infty}$ .

  - p = 1, linear convergence. 
  - p > 1, super-linear convergence. 
  - p = 2, quadratic convergence.

  > If you have an **iterative method for solving the root of the equation**:
  >
  > - linear convergence: the error you reduce with each iteration is relatively fixed. 
  > - superlinear or quadratic convergence: the error you reduce accelerates with each iteration
  >   - In Newton's method, it is usually quadratically convergent, meaning that the error decreases in square with each iteration, which makes it very efficient as it approaches the root

- **Theorem**: ${\{x_n\}}^∞_{n=0}$ super-linearly converges to $x_∞$, then when $x_n \neq x_∞$, $\lim\limits_{n \rightarrow ∞} \frac{||x_{n+1}−x_n||}{||x_{n}−x_∞||^p}=1$.

  - Proof: 

    ​					 $$\lim_{n \rightarrow ∞} |\frac{||x_{n+1}−x_n||}{||x_{n}−x_∞||}-1|=\lim_{n \rightarrow ∞} |\frac{||x_{n+1}−x_n||-||x_{n}−x_\infty||}{||x_{n}−x_∞||}| \\  \leq \frac{||x_{n+1}−x_\infty||}{||x_{n}−x_∞||}=0$$

    > This indicates that when $ n $ is large enough, the distance of the subsequent term $ x_{n+1} $ from the limit $ x_{\infty} $ is smaller than that of the previous term $ x_n $.

    When a sequence is super-linear convergence, we can use $||x_{n+1}−x_n|| < \epsilon$ as a stopping rule.

    For Newton's method, let $M(x) \ be \ x- \frac{g(x)}{g'(x)} $.

    ​									 $$M'(x) = 1 - \frac{g'(x)}{g'(x)} + \frac{g(x)g''(x)}{g'(x)^2} = \frac{g(x)g''(x)}{g'(x)^2}$$

    ​										 $$M'(x_\infty) = \frac{g(x_\infty)g''(x_\infty)}{g'(x_\infty)^2} = 0$$

    The last equation holds since $g(x_\infty) = 0$.

    ​						 $$x_n - x_\infty = M(x_{n-1}) - M(x_\infty)\\ = ( \text{Taylor expansion} ) M'(x_\infty)(x_{n-1} - x_\infty) + \frac{1}{2} M''(z_n)(x_{n-1} - x_\infty)^2\\= \frac{1}{2} M''(z_n)(x_{n-1} - x_\infty)^2$$ 

    It follows that

    ​							 $$\lim\limits_{n \to \infty} \frac{\| x_n - x_\infty \|}{\| x_{n-1} - x_\infty \|^2} = \lim\limits_{n \to \infty} \frac{1}{2} M''(z_n) = \frac{1}{2} M''(x_\infty)$$

    Therefore, <u>the Newton sequence is quadratically convergent</u>.

    Example: Given \( a \), we need to find $ \frac{1}{a}$. Construct $g(x) = a - \frac{1}{x}$, then the <u>Newton iteration</u> is $x_{n+1} = x_n(2-ax_n)$.


## 2.5. Multivariate Case

- Newton’s method for a mapping $\vec{F}$ (e.g. $\vec{F}: \mathbb{R^3} \rightarrow \mathbb{R^3}$ ). 

- We consider the mapping $\vec{F}(\vec{x})$ from a $\mathbb{R}^m$ domain $D$ to $\mathbb{R}^m$, where $\vec{x} = (x_1, x_2, \ldots, x_m)$, and $ \vec{F}(\vec{x}) = (f_1(\vec{x}), f_2(\vec{x}), \ldots, f_m(\vec{x}))$. Our goal is to solve the equation system  $ \vec{F}(\vec{x}) = \vec{0}. $ 

- Given current point $\vec{x}^{(n)}$, we carry out **taylor expansion** for $f_i(\vec{x})$ ($i = 1, \ldots, m$) at $\vec{x}^{(n)}$, 

  ​							 $$f_j(x) \approx f_j(\vec{x}^{(n)}) + \frac{\partial f_j}{\partial x_1}(\vec{x}^{(n)}) (x_1 - x^{(n)}_1) + \cdots + \frac{\partial f_j}{\partial x_m}(\vec{x}^{(n)}) (x_m - \vec{x}^{(n)}_m).$$

  The equation above holds for $i = 1, . . . , m$. We put these m equations together, which become
  $$
  \vec{F}(\vec{x}) \approx \vec{F}(\vec{x}^{(n)}) + \vec{F}'(\vec{x}^{(n)})(\vec{x} - \vec{x}^{(n)}),
  $$
  where the **Jacobian matrix** of $\vec{F}$ is

  ​									 $$F'(\vec{x}_n) = \begin{pmatrix}
  \frac{\partial f_1}{\partial x_1}(\vec{x}^{(n)}) & \cdots & \frac{\partial f_1}{\partial x_m}(\vec{x}^{(n)}) \\
  \vdots & \ddots & \vdots \\
  \frac{\partial f_m}{\partial x_1}(\vec{x}^{(n)}) & \cdots & \frac{\partial f_m}{\partial x_m}(\vec{x}^{(n)})
  \end{pmatrix}$$

  and

  ​										 $$(\vec{x} - \vec{x}^{(n)}) = \begin{pmatrix}
  x_1 - x_1^{(n)} \\
  \vdots \\
  x_m - x_m^{(n)}
  \end{pmatrix}$$

  Let the left hand side of equation (3) be zero. It yields that

  ​									 $$ \vec{x}^{(n+1)} = \vec{x}^{(n)} - \left( \vec{F}'(\vec{x}^{(n)}) \right)^{-1} \vec{F}(\vec{x}^{(n)}).$$

  

​	The equation above can be decomposed to two steps:

​		- Solve $ \vec{F}'(\vec{x}_n) \Delta \vec{x}^{(n)} = -\vec{F}(\vec{x}^{(n)}); $

​		- $ {x}^{(n+1)} = {x}^{(n)} + \Delta {x}^{(n)}.$

- Example (calculate MLE): $ l(\theta | x_1, \ldots, x_n) = \log L(\theta | x_1, \ldots, x_n) $. Under some regular conditions, $ \hat{\Theta} $ solves the following equation, 

​												$$ \begin{pmatrix} \frac{\partial}{\partial \theta_1} \\* \vdots \\ \frac{\partial}{\partial \theta_m} \end{pmatrix} = \begin{pmatrix} 0 \\ \vdots \\ 0 \end{pmatrix}$$.

By Newton’s method, we iteratively update the $Θ^{(n)}$ according to
$$
\Theta^{(n+1)} = \Theta^{(n)} - \left( 
\begin{array}{cccc}
\frac{\partial^2 l}{\partial \theta_1 \partial \theta_1^{}}(\Theta^{(n)}) & \cdots & \frac{\partial^2 l}{\partial \theta_1 \partial \theta_m}(\Theta^{(n)}) \\
\vdots & \ddots & \vdots \\
\frac{\partial^2 l}{\partial \theta_m \partial \theta_1^{}}(\Theta^{(n)}) & \cdots & \frac{\partial^2 l}{\partial \theta_m \partial \theta_m}(\Theta^{(n)})
\end{array}
\right)^{-1}\left( 
\begin{array}{c}
\frac{\partial l}{\partial \theta_1^{}}(\Theta^{(n)}) \\
\vdots \\
\frac{\partial l}{\partial \theta_m}(\Theta^{(n)})
\end{array}
\right)
$$
Example (MLE of Poisson distribution):

​										 $$ f(y_1, \ldots, y_n | \lambda) = \prod_{i=1}^{n} \frac{e^{-\lambda} \lambda^{y_i}}{y_i!}\\
l(\lambda | y_1, \ldots, y_n) = \sum_{i=1}^{n} \left( y_i \log \lambda - \lambda - \log y_i! \right)\\
\       = \left( \sum_{i=1}^{n} y_i \right) \log \lambda - n \lambda - \sum_{i=1}^{n} \log y_i!\\
\frac{d l}{d \lambda} =  \frac{\sum_{i=1}^{n}y_i}{\lambda} - n$$

​	MLE direct derivation: $\hat{\lambda} = \frac{\sum_{i=1}^{n} y_i}{n} $.

​	Newton's method to solve: $\lambda_{k+1} = \lambda_k + \frac{{\lambda}^2_k}{\sum_{i=1}^{n} y_i} ( \frac{\sum_{i=1}^{n} y_i}{\lambda_k} - n )$.

- e.g. Poisson regression

  - We have independent count data ${y_1, . . . , y_n}$. For each $Y_i$ , $Y_i$ follows $Poi(λ_i)$, where $log(λ_i) = α + βx_i$ , $α$ and $β$ are parameters and xi is the fixed covariate. The p.d.f (probability density function) of $y_i$ is $f(y_j | \alpha, \beta, x_i) = e^{-e^{(\alpha + \beta x_i)}} \frac{(e^{(\alpha + \beta x_i)})^{y_i}}{y_i!}$. It follows that the joint p.d.f. is

    ​         	   $f(y_1, y_2, \ldots, y_n | \alpha, \beta) = \prod\limits_{i=1}^{n} e^{-e^{(\alpha + \beta x_i)}} \frac{(e^{\alpha + \beta x_i})^{y_i}}{y_i!}.$ 

    $$l(\alpha, \beta) = \log f(y_1, y_2, \ldots, y_n | \alpha, \beta) = \sum\limits_{i=1}^{n} [-e^{\alpha + \beta x_i} +y_i(\alpha+\beta x_i)- \log y_i!] $$ 

    ​						$$ \frac{\partial l}{\partial \alpha} = -\sum_\limits{i=1}^{n} e^{\alpha + \beta x_i} + \sum_\limits{i=1}^{n} y_i $$ 

    ​						$$ \frac{\partial l}{\partial \beta} = -\sum_\limits{i=1}^{n} x_i e^{\alpha + \beta x_i} + \sum\limits_{i=1}^{n} x_i y_i $$ 

    ​						$$ \frac{\partial^2 l}{\partial \alpha^2} = -\sum_{i=1}^{n} e^{\alpha + \beta x_i} $$ 

    ​						$$ \frac{\partial^2 l}{\partial \alpha \partial \beta} = -\sum\limits_{i=1}^{n} x_i e^{\alpha + \beta x_i} $$

    ​						$$\frac{\partial^2 l}{\partial \beta^2} = -\sum\limits_{i=1}^{n} x_i^2 e^{\alpha + \beta x_i} $$ 

    The Newton step is

    $\begin{pmatrix}  \alpha_{k+1} \\  \beta_{k+1}  \end{pmatrix} 
    =  \begin{pmatrix}  \alpha_k \\  \beta_k  \end{pmatrix} -  \begin{pmatrix}  -\sum_{i=1}^{n} e^{\alpha_k + \beta_k x_i} & -\sum_{i=1}^{n} x_i e^{\alpha_k + \beta_k x_i} \\  -\sum_{i=1}^{n} x_i e^{\alpha_k + \beta_k x_i} & -\sum_{i=1}^{n} x_i^2 e^{\alpha_k + \beta_k x_i}  \end{pmatrix}^{-1}  \begin{pmatrix}  -\sum_{i=1}^{n} e^{\alpha_k + \beta_k x_i} + \sum_{i=1}^{n} y_i \\  -\sum_{i=1}^{n} x_i e^{\alpha_k + \beta_k x_i} + \sum_{i=1}^{n} x_i y_i  \end{pmatrix}$ 

    

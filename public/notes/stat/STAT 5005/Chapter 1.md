# Chapter 1: Measurable Theory

[TOC]

## 1. Probability Space $(\Omega,\mathcal F,P)$

- **Examples of Probability Space**
  - Toss a coin
    - $\Omega=\{H,T\}$
    - $\mathcal F=\{\varnothing,\Omega,\{H\},\{T\}\}$
    - $P:\mathcal F\rightarrow [0,1]$, $P(H)=P(T)=0.5$, $P(\Omega)=1, P(\varnothing)=0$
  - Borel $\sigma$-field
    - $\Omega=\mathbb R$, 
    - $\mathcal F=\mathcal B$ (if $2^\Omega$, then the collection of sets could be so large that defining $P$ is hard)
    - $P: \mathcal F\rightarrow [0,1]$ (fundamentally defined as $P(-\infty,x]$, i.e., cdf)

### 1.1 Definitions and Properties

#### 1.1.1 Sample space and $\sigma$-field

- **Definition**. A sample space, denoted by $\Omega$, is a set (of “outcomes”)

- **Definition**. A collection of subsets of $\Omega$, denoted by F, is called a $\sigma$-field or $\sigma$-algebra if:
  $$
  \begin{align}
  &\text{(i) }\Omega\in\mathcal{F},\\
  &\text{(ii) If }A\in\mathcal{F},\text{ then }A^c\in\mathcal{F},\\
  &\text{(iii) If }A_1,A_2,\cdots\in\mathcal{F},\text{ then }\cup_{i=1}^\infty A_i\in\mathcal{F}.\end{align}
  $$

  - Elements of $\mathcal F$ are called **events**
  - $\{\varnothing,\Omega\}$ is smallest $\sigma$-field while $2^\Omega$ is the largest
  - $\{\varnothing,\Omega, A, A^c\}$ is also a $\sigma$-field (generated by $A$)

- **Fact about $\sigma$-field**. 

  - Finite countably unions: $A_1,...,A_n\in\mathcal F\Rightarrow \cup_{i=1}^n A_n\in\mathcal F$ (letting $A_{n+1},A_{n+2},...=\varnothing$)
  - Countably intersection closure: $A_1,A_2,\cdots\in\mathcal{F}\Rightarrow \cap_{i=1}^\infty A_i\in\mathcal{F}$ (think of Demorgan's Law and let $\cap_{i=1}^\infty A_i=[\cup_{i=1}^\infty A^c]^c$ and apply $(iii)$ above)

  - If $\mathcal F_i,i\in I$ are all $\sigma$-field, then $\cap_{i\in I} \mathcal F_i$ are also $\sigma$-field (think of three properties of $\sigma$-field)

- **Definition**. The above $(\Omega, \mathcal F)$ is called a measurable space

#### 1.1.2 Measures

- **Definition**. $\mu: \mathcal F\rightarrow \mathbb R$ is called a measure if $\mu(A)\ge 0,\forall A\in\mathcal F$
  $$
  \begin{aligned}
  &(1)\:\mu(A)\geq0,\:\forall\:A\in\mathcal{F},\\
  &(2)\:\mu(\varnothing)=0,\\
  &(3)\mathrm{~If~}A_1,A_2,\cdots\in\mathcal{F}\text{ are disjoint, then }\mu(\cup_{i=1}^\infty A_i)=\sum_{i=1}^\infty\mu(A_i).
  \end{aligned}
  $$

  - The (2) and (3) implies $\mu(\cup_{i=1}^n A_i)=\sum_{i=1}^n\mu(A_i)$ by letting $A_{n+1},A_{n+2},...=\varnothing$

- **Definition**: If $\exists A_i\uparrow \Omega$ with $\mu(A_i)<\infty$, then $\mu$ is called a $\sigma$-finite measure

  - If $\mu(\Omega)<\infty$, then $\mu$ is called a finite measure (which implies $\sigma$-finite measure)
  - If $\mu(\Omega)=1$, then $\mu$ is called a probability measure

- **Properties of Measures and Proof**
  $$
  \begin{array}{ll}\text{(a) If }A\subset B,\text{ then }\mu(A)\leq\mu(B).&\text{(Monotonicity.)}\\
  \text{(b) }\forall A,B,\:\mu(A)+\mu(B)=\mu(A\cup B)+\mu(A\cap B).&\text{(Addition law.)}\\
  \text{(c) }\mu(\cup_{i=1}^\infty A_i)\leq\sum_{i=1}^\infty\mu(A_i).&\text{(Sub-additivity.)}\\
  \text{(d) If }A_n\uparrow A,\text{ then }\mu(A_n)\uparrow\mu(A).&\text{(Continuity from below.)}\\
  \text{(e) If }A_n\downarrow A\text{ and }\mu(A_1)<\infty,\text{ then }\mu(A_n)\downarrow\mu(A).&\text{(Continuity from above.)}\end{array}
  $$

  - Key point to prove: decompose some sets into disjoint sets unions to apply (3) of definition

  - (a). $\mu(B)=\mu(A\cup(B\backslash A))\overset{(3)}=\mu(A)+\mu(B\backslash A)\overset{(1)}\ge \mu(A)$

  - (b). Decompose sets as:
    $$
    \begin{cases}
    A\cup B&= (A\backslash B)\cup(B\backslash A)\cup(A\cap B)\\
    A&=(A\backslash B)\cup (A\cap B)\\
    B&=(B\backslash A)\cup (A\cap B)\\
    \end{cases}
    $$

    $$
    \Downarrow\\
    \begin{cases}
    \mu(A\cup B)&\overset{(3)}= \mu(A\backslash B)\cup \mu(B\backslash A)\cup\mu(A\cap B)\\
    \mu(A)&\overset{(3)}=\mu(A\backslash B)\cup \mu(A\cap B)\\
    \mu(B)&\overset{(3)}=\mu(B\backslash A)\cup \mu(A\cap B)\\
    \end{cases}\\
    \Downarrow\\
    \mu(A)+\mu(B)=\mu(A\cup B)+\mu(A\cap B)
    $$

  - (c). Write $\cup_{i=1}^\infty A_i= A_1\cup (A_2\backslash A_1)\cup (A_3\backslash (A_1\cup A_2))\cup \cdots$, then the measure is:
    $$
    \begin{align}
    \mu(\cup_{i=1}^\infty A_i)&=\mu(A_1\cup (A_2\backslash A_1)\cup (A_3\backslash (A_1\cup A_2))\cup \cdots)\\
    &\overset{(3)}=\mu(A_1)+\mu(A_2\backslash A_1)+\mu(A_3\backslash (A_1\cup A_2))+\cdots\\
    &\overset{(a)}\le \mu(A_1)+\mu(A_2)+\mu(A_3)+\cdots\\
    &=\sum_{i=1}^\infty\mu(A_i)
    \end{align}
    $$
    Note that if we have $A\subset\cup_{i=1}^\infty A_i$, then $\mu(A)\overset{(a)}\le\sum_{i=1}^\infty\mu(A_i)$

  - (d). Before proving, please note that $A_n\uparrow A$ means $A_1\subset A_2\subset\cdots$ and $\cup_{n=1}^\infty A_n=A$ (so is $A_n\downarrow A$), and $\mu(A_n)\uparrow\mu(A)$ means $\mu(A_1)\le \mu(A_2)\le\cdots$ and $\lim_{n\rightarrow\infty}\mu(A_n)=\mu(A)$. Non-decreasing property of $\{\mu(A_n)\}_{n\in \mathbb N^+}$ is easy to prove. So we prove it by:
    $$
    \begin{align}
    \mu(A)=\mu(\cup_{n=1}^\infty A_n)&=\mu(A_1\cup(A_2\backslash A_1)\cup(A_3\backslash A_2)\cup\cdots)\\
    &\overset{(3)}=\mu(A_1)+\mu(A_2\backslash A_1)+\mu(A_3\backslash A_2)+\cdots\\
    &=\lim_{n\rightarrow\infty}\sum_{i=1}^n\mu(A_i\backslash A_{i-1})\quad\text{(let $A_0=\varnothing$)}\\
    &=\lim_{n\rightarrow\infty}\mu(A_n)\\
    &\text{(because$A_i\backslash A_{i-1}$ are disjoint and we use inverse of (3))}
    \end{align}
    $$
    Note that the key point here is convert infinite operations into finite operations and then back to infinite operations

  - (e). we know $(A_1\backslash A_n)\uparrow (A_1\backslash A)$. So use (d), we have $\mu(A_1\backslash A_n)\downarrow \mu(A_1\backslash A)$, 

    - i.e., $\mu(A_1\backslash A)=\lim_{n\rightarrow\infty}\mu(A_1\backslash A_n)$
    - $A_1=(A_1\cap A)\cup (A_1\backslash A)\overset{(3)}\Rightarrow LHS=\mu(A_1\backslash A)=\mu(A_1)-\mu(A_1\cap A)=\mu(A_1)-\mu(A)$
    - Similarly, $RHS=\lim_{n\rightarrow\infty}[\mu(A_1)-\mu(A_n)]=\mu(A_1)-\lim_{n\rightarrow\infty}[\mu(A_n)]$
    -  So we have $\mu(A)=\lim_{n\rightarrow\infty}[\mu(A_n)]$

    

### 1.2 $\sigma(\mathcal A)$ & Borel $\sigma$-field

- **Definition**. Let $\mathcal A$ be a collection of subsets of $\Omega$, then define $\sigma$-field as $\sigma(\mathcal A):=\underset{\mathcal A\subset\mathcal F_i,\mathcal F_i\text{ is} \sigma\text{-field for all }i\in I}\cap \mathcal F_i$. 

  - Simply put it, $\sigma(\mathcal A)$ is smallest $\sigma$-algebra containing $\mathcal A$
  - We also call $\sigma(\mathcal A)$ as $\sigma$-algebra generated by $\mathcal A$
  - Why $\cap \mathcal F_i$ is still a $\sigma$-algebra: proved before 
  - If $\mathcal A=\{A\}$, then $\sigma(\mathcal A)=\{\Omega,\varnothing, A,A^c\}$

- **Definition**. Suppose $\Omega=\mathbb R$, $\mathcal A=\{(a,b]:-\infty<a\le b<\infty\}$, then $\sigma(\mathcal A)$ is Borel $\sigma$-field, denoted by $\mathcal B$ or $\mathcal R$

- **Fact**. When $\mathcal A^\prime=\{(a,b):-\infty<a\le b<\infty\}$, then $\sigma(\mathcal A^\prime)=\sigma(\mathcal A)=\mathcal B$. Proof:

  - $\sigma(\mathcal A)\subset\sigma(\mathcal A^\prime)$:
    - To prove it, we only need to prove $\mathcal A\subset\sigma(\mathcal A^\prime)$ (because if that is true, then both $\sigma(\mathcal A)$ and  $\sigma(\mathcal A^\prime)$ are $\sigma$-algebra containing $\mathcal A$ and the former is smallest, so it must be smaller than the latter one)
    - Note that for any $(a,b]\in\mathcal A$, we can express them with $(a,b]=\cap_{n=1}^\infty(a,b+\frac1n)$, then since the RHS is countable intersections of elements in $\sigma(\mathcal A^\prime)$, then RHS is also in $\sigma(\mathcal A^\prime)$, so $(a,b]\in\mathcal A^\prime$
  - $\sigma(\mathcal A^\prime)\subset\sigma(\mathcal A)$
    - NTS $\mathcal A^\prime\subset\sigma(\mathcal A)$
    - for any $(a,b)\in\mathcal A$, we can express them with $(a,b)=\cup_{n=1}^\infty(a,b-\frac1n]$, which is countable unions of elements in $\sigma(\mathcal A)$, then $(a,b)\in\mathcal A$
  - Then $\sigma(\mathcal A^\prime)=\sigma(\mathcal A)=\mathcal B$. We can prove $\mathcal B=\sigma(\mathcal A^{\prime\prime})$, where $\mathcal A^{\prime\prime}=\{(-\infty,a):-\infty<a<\infty\}$ too.

- **Definition**. Borel $\sigma$-field on Euclidean space $\Omega=\mathbb R^d$ is denoted as $\mathcal B$ or $\mathcal R^d$ and defined as: 
  $$
  \mathcal{B}=\sigma(\{(a_1,b_1]\times(a_2,b_2]\times\cdots\times(a_d,b_d]:-\infty<a_i<b_i<\infty\})
  $$
  



### 1.3 Stieltjes Measure Function Determines Unique Measure on Bore $\sigma$-field

- **Definition**. $F:\mathbb R\Rightarrow\mathbb R$ is called a Stieltjes measure function if:
  $$
  \begin{aligned}&\text{(i) }F\text{ is nondecreasing,}\\&\text{(ii) }F\text{ is right-continuous, i.e., }\lim_{y\downarrow x}F(y)=F(X).\end{aligned}
  $$

- **Theorem**. Every Stieltjes measure function $F$ determines a unique measure $\mu$ on $(\mathbb R,\mathcal B)$ such that:
  $$
  \mu((a,b])=F(b)-F(a),\quad\forall\:-\infty<a<b<\infty.
  $$

  - **There are two parts of this theorem**: $F$ determines a measure $\mu((a,b])=F(b)-F(a)$ & and this measure is unique
  - To prove the measure is $\mu((a,b])=F(b)-F(a)$, we need inverse transformation between Stieltjes function and random variable
  - To prove the measure is unique, we need Dynkin's $\pi$-$\lambda$ Theorem

- The remaining part focus on proof of the theorem

#### 1.3.1 Proof of Measure Form

- **Youtube Reference**: [Proof of Lebesgue-Stieltjes Measure](https://www.youtube.com/watch?v=uaJ_dHAmdu4&t=1s)

- **Preparations**

  - Let $F(\infty)=\lim_{x\rightarrow\infty} F(x),F(-\infty)=\lim_{x\rightarrow-\infty} F(x)$
  - Define an open interval as $(F(-\infty),F(\infty))$ (treat it as cdf range, i.e., from 0 to 1)
  - Define $g(y)=\inf\{x\in\mathbb R:y\le F(x)\}$

- We can prove $g$ is left-continuous and non-decreasing (note how $g(y)$ works, more proof details also in the above Youtube reference).

  <img src="CDF_Inverse.png" style="zoom: 33%;" />

- Set a random variable $X=g(Y)$
  - When considering measure of $X$ on $(a,b]$, we are actually considering measure of $Y$ on $g^{-1}((a,b])=(F(a),F(b)]$
  - Since we assign Lebesgue measure on $F(x)$, then the measure is $F(b)-F(a)$
  - We can see that $g$ is Borel measurable, which means given Borel subset in $X$, $Y=F(X)$ is also in Borel subset. That could make sure the measure transformation is well defined

#### 1.3.2 Dynkin‘s $\pi$-$\lambda$ Theorem

- **Definition**: If $\mathcal P$ is a $\pi$-system, $\mathcal L$ is a $\lambda$-system, and $\mathcal P\subset \mathcal L$, then $\sigma(\mathcal P)\subset\mathcal L$

1. **Basic Definitions and Fact**

- **Definition**: $\mathcal L$ is a $\lambda$-system if:
  
  - (1) $\Omega\in\mathcal L$
  - (2) If $A,B\in\mathcal L$ and $A\subset B$, then $B\backslash A\in\mathcal L$
  - (3) If $A_1,A_2,…\in\mathcal L$ and $A_i\uparrow A$, then $A\in\mathcal L$
  
- **Definition**: $\mathcal P$ is a $\pi$-system if:
  
  - $A,B\in\mathcal{P}\Longrightarrow A\cap B\in\mathcal{P}$
  
- **Lemma**: if $\mathcal F$ is both a $\lambda$-system and a $\pi$-system, then $\mathcal F$ is a $\sigma$-field. Proof:

  - (1) is same for $\lambda$-system and $\sigma$-algebra, while (2) of $\sigma$-algebra could be easily verified by letting $B=\Omega$ in (2) of $\lambda$-system

  - So we need to verify (3) of $\sigma$-algebra: if $A_1,A_2,...\in\mathcal F$, then $\cup_{i=1}^\infty A_i\in\mathcal F$

  - This could be verified by:
    $$
    \cup_{i=1}^\infty A_i=A_1\cup(A_1\cup A_2)\cup(A_1\cup A_2\cup A_3)\cup\cdots,\quad A_1\cup A_2=(A_1^c\cap A_2^c)^c
    $$
    and by $\pi$-system, we know $A_1\cup A_2$, $A_1\cup A_2\cup A_3$, ..., are all in $\mathcal F$; Also, let $B_1=A_1,B_2=A_1\cup A_2,B_3=A_1\cup A_2\cup A_3,...$, then $\cup_{i=1}^\infty B_i\uparrow B$, where $B=\cup_{i=1}^\infty A_i$, so with $\lambda$-system, we have $B=\cup_{i=1}^\infty A_i\in\mathcal F$

2. **Dynkin‘s $\pi$-$\lambda$ Theorem Proof**

- Define $ l(\mathcal P)$ to be the smallest $\lambda$-system containing $\mathcal P$, so $\mathcal P\subset l(\mathcal P)\subset\mathcal L$
  -  $\mathcal P\subset l(\mathcal P)$ is because $ l(\mathcal P)$ contains $\mathcal P$
  - $ l(\mathcal P)\subset\mathcal L$ is because they are both $\lambda$-systems containing $\mathcal P$ and the former one is smallest $\lambda$-system
  
- If we can show $ l(\mathcal P)$ is a $\pi$-system, then the proof is done
  - Since we already know $ l(\mathcal P)$ is a $\lambda$-system, then if we can prove $ l(\mathcal P)$ is a $\pi$-system means $ l(\mathcal P)$ is a $\sigma$-algebra (by above **Lemma**)
  - If $ l(\mathcal P)$ is $\sigma$-algebra, then we know $\sigma(\mathcal P)\subset l(\mathcal P)$ because the former is smallest $\sigma$-algebra containing $\mathcal P$
  - Take above, we have $\sigma(\mathcal P)\subset l(\mathcal P)\subset\mathcal L$, so $\sigma(\mathcal P)\subset\mathcal L$
  
- Now let $\mathcal G_A=\{B:\; A\cap B\in l(\mathcal P)\}$, and we want to prove $A\in l(\mathcal P)\Rightarrow \mathcal G_A$ is a $\lambda$-system. It could be verified by the definition of $\lambda$-system

  -  $A\cap \Omega=A\Rightarrow \Omega\in\mathcal G_A$
  - if $B,C\in\mathcal G_A$ and $B\supset C$, then:
    -  $A\cap B\in l(\mathcal P),\; A\cap C\in l(\mathcal P)$;  (because $B,C\in\mathcal G_A$)
    - $A\cap (B\backslash C)=(A\cap B)\backslash (A\cap C)$ (because $B\supset C$) is also in $l(\mathcal P)$ because $l(\mathcal P)$ is $\lambda$-system


  - if $B_n\in\mathcal G_A$ and $B_n\uparrow B$, then $A\cap B_n\in l(\mathcal P)$ for each $n$ and since $l(\mathcal P)$ is a $\lambda$-system, we have $A\cap B_n\uparrow A\cap B$

- Now we prove $A\in \mathcal P\Rightarrow l(\mathcal P)\subset \mathcal G_A$. So suppose $A\in \mathcal P$, then:

  - $A\in \mathcal P$ implies $A\in l(\mathcal P)$ (because $l(\mathcal P)$ contains $\mathcal P$), which further implies $\mathcal G_A$ is a $\lambda$-system by previous result
  - Since $\mathcal P$ is a $\pi$-system, then $\forall C\in\mathcal P$ we have $A\cap C\in\mathcal P\subset l(\mathcal P)$. This $\forall$ statement further means $\mathcal P\subset \mathcal G_A$
  - Now since $\mathcal G_A$ is a $\lambda$-system and $\mathcal P\subset \mathcal G_A$, while $l(\mathcal P)$ is smallest $\lambda$-system containing $\mathcal P$, then $l(\mathcal P)\subset \mathcal G_A$

- Lastly, we derive the result

  - $A\in \mathcal P\Rightarrow l(\mathcal P)\subset \mathcal G_A$ implies if $A\in\mathcal P$ and $B\in l(\mathcal P)$, then $\mathcal P\subset \mathcal G_B$
    - Reason: Since $A\in\mathcal P$ means $l(\mathcal P)\subset \mathcal G_A$, so if we let $B\in l(\mathcal P)$, which means $B\in \mathcal G_A$ as well, so by definition of $\mathcal G_A$, we have $A\cap B\in l(\mathcal P)$
    - Implication: the statement implies if $B\in l(\mathcal P)$, then $\mathcal P\subset \mathcal G_B$ (because $\forall A\in\mathcal P$ we have $A\cap B\in l(\mathcal P)$), and thus $l(\mathcal P)\subset\mathcal G_B$ (because both of them are $\lambda$ systems containing $\mathcal P$)

  - Now we get the final important statement: if $B\in l(\mathcal P)$, then $l(\mathcal P)\subset\mathcal G_B$
    - equivalently, $\forall B\in l(\mathcal P)$, we know $l(\mathcal P)\subset\mathcal G_B$, and when considering $\forall A\in l(\mathcal P)$, we know these $A$ also contained in  $\mathcal G_B$
    - Thus by definition of $\mathcal G_B$: $\forall B\in l(\mathcal P)$, we have $A\cap B\in l(\mathcal P)$ for all $A\in l(\mathcal P)$

#### 1.3.3 Proof of Uniqueness

- **Lemma**: suppose $\mathcal P$ is a $\pi$-system in $\mathbb R$, and $\mu$ and $\nu$ are probability measures on $(\mathbb R,\sigma(\mathcal P))$ such that $\mu,\nu$ agree on $\mathcal P$. (i.e., $\mu(A)=\nu(A),\forall A\in\mathcal P$). Then $\mu$ agrees with $\nu$ on $\sigma(\mathcal P)$ (i.e., $\mu(A)=\nu(A),\forall A\in\mathcal\sigma(P)$).

- Essence: 

  - our interested collection of sets $\mathcal{P}:=\{(a,b]:-\infty<a\leq b<\infty\}$ is a $\pi$-system, which is starting collection of sets to construct Borel $\sigma$-algebra $\mathcal B=\sigma(\mathcal P)$. 
  - So if we want there is only one unique measure on $\mathcal B=\sigma(\mathcal P)$, we only need to prove for all $B\in \sigma(\mathcal P)$, $\mu(B)=\nu(B)$
  - That is equivalent to show: $\sigma(\mathcal P)\subset \mathcal L$, where $\mathcal L:=\{B\in\sigma(\mathcal P):\mu(B)=\nu(B)\}$

- **Proof**

  - From condition that $\mu,\nu$ agree on $\mathcal P$. (i.e., $\mu(A)=\nu(A),\forall A\in\mathcal P$), we know $\mathcal P\subset \mathcal L$

  - Since  $\mathcal P$ is a $\pi$-system, if we prove $\mathcal L$ is a $\lambda$-system, then Dynkin‘s $\pi$-$\lambda$ Theorem tell us $\sigma(\mathcal P)\subset \mathcal L$

  - Now verify $\mathcal L$ is a $\lambda$-system

    - Since $\mu(\mathbb R)=\nu(\mathbb R)$, then $\mathbb R\in\mathcal L$

    - If $A,B\in\mathcal L$ and $A\subset B$, then 
      $$
      \mu(A)=\nu(A),\mu(B)=\nu(B)\\
      \Downarrow\\
      \mu(B)=\mu(B\backslash A)+\mu(A)=\nu(B\backslash A)+\nu(A)=\nu(B)\\
      \Downarrow\\
      \mu(B\backslash A)=\nu(B\backslash A)
      $$

    - If $B_n\in\mathcal L$ and $B_n\uparrow B$, then $\mu(B_n)=\nu(B_n)$, and because above, we have $\mu(B_n\backslash B_{n-1})=\nu(B_n\backslash B_{n-1})$. Then we have:
      $$
      \begin{align}
      \mu(B)&=\mu(B_1\cup(B_1\backslash B_2)\cup(B_3\backslash B_2)\cup\cdots)\\
      &=\mu(B_1)+\mu(B_1\backslash B_2)+\mu(B_3\backslash B_2)+\cdots\\
      &=\nu(B_1)+\nu(B_1\backslash B_2)+\nu(B_3\backslash B_2)+\cdots\\
      &=\nu(B_1\cup(B_1\backslash B_2)\cup(B_3\backslash B_2)\cup\cdots)\\
      &=\nu(B)
      \end{align}
      $$
      so $B\in\mathcal L$




## 2. Random Variables $X$ and Distributions $\mathcal L(X)$

- **Motivation**. 

  - Random variable is $X:\Omega\rightarrow\mathbb R$

  - Distribution: 
    $$
    \mathcal L(X)=\begin{cases}
    P(X=x)=P\{\omega\in\Omega:X(\omega)=x\}\\
    P(X\le x)=P\{\omega\in\Omega:X(\omega)\le x\}
    \end{cases}
    $$

  - Note when defining distribution, we concentrate on events in $\mathcal F$

### 2.1 Measurable function

- **Definition**. Let $(\Omega_1,\mathcal{F}_1)$ and $(\Omega_2,\mathcal{F}_2)$ be measurable spaces. $f:\Omega_1\to\Omega_2$ is called measurable if for any $A\in\mathcal{F}_2,f^{-1}(A)\in\mathcal{F}_1$, where $f^{-1}(A)=\{w_1\in\Omega_1:f(w_1)\in A\}$

  - Note $f:\Omega_1\to\Omega_2$, which means for every element in $\Omega_1$, $f$ will map it to one value in $\Omega_2$. There could be some values in $\Omega_2$ that is cannot be mapped to by elements in $\Omega_1$, and some values in $\Omega_2$ could possibly be mapped to by  more than one elements in $\Omega_1$
  - Measurable function is defined through $\sigma$-field, so that later on distribution could be defined
  - Implication: 
    - If $A\in\mathcal F_1,B\in\mathcal F_2,A^c=\Omega_1\backslash A, B^c=\Omega_2\backslash B$, and there is preimage relation as $A=f^{-1}(B)$, then $A^c=f^{-1}(B^c)$
    - If $A_i,\in\mathcal F_1,B_i\in\mathcal F_2,i=1,...$, and there is preimage relation as $A_i=f^{-1}(B_i)$, then $\cup_{i=1}^\infty A_i=f^{-1}(\cup_{i=1}^\infty B_i)=\cup_{i=1}^\infty f^{-1}(B_i)$

- **Fact**. If $\mathcal F_1=2^{\Omega_1}$ or $\mathcal F_2=\{\varnothing,\Omega_2\}$, then any function is measurable.

  - $\mathcal F_1=2^\Omega$, the power set, make any function measurable because no matter what $\mathcal F$ is, the preimage is always some events shaped by $\omega\in\Omega_1$ in the way of $\sigma$-algebra
  -  $\mathcal F_2=\{\varnothing,\Omega_2\}$ is because the preimage is always $\Omega_1,\varnothing$

- **Fact**. $\sigma(f)=\{f^{-1}(B):B\in\mathcal{F}_2\}$ is a $\sigma$-field in $\Omega_1$

  - We could verify $\sigma(f)$ is a $\sigma$-field:
    - Since $\Omega_2\in\mathcal F_2$ and $f^{-1}(\Omega_2)=\Omega_1$, then we have $\Omega_1\in\sigma(f)$
    - If $A\in\sigma(f)$, then $A=f^{-1}(B),B\in\mathcal F_2\Rightarrow A^c=f^{-1}(B^c)$ where $A^c=\Omega_1\backslash A,B^c=\Omega_2\backslash B, B^c\in\mathcal F_2$. We can think of how $f^{-1}(B)$ works. So $A^c\in\sigma(f)$
    - If $A_i,i=1,...\in \sigma(f)$, then $A_i=f^{-1}(B_i),i=1,...$. So $\cup_{i=1}^\infty A_i=f^{-1}(\cup_{i=1}^\infty B_i)$. And since $B_i\in\mathcal F_2,$, then $\cup_{i=1}^\infty B_i\in\mathcal F_2$, so $\cup_{i=1}^\infty A_i\in\sigma(f)$


  - We call $\sigma(f)$ the $\sigma$-field generated by $f$, and it is the smallest $\sigma$-field in $\Omega_1$ to make $f$ measurable. (think of some events in $\sigma(f)$, actually they could be further divided into more 'small' events)
  - Example of $\sigma(f)$: 
    - Let $f(\omega)=a,\forall \omega\in\Omega_1$, then $\sigma(f)=\{\varnothing,\Omega\}$
    - Let $A,A^c\in\Omega_1$, and let $f(\omega)=\begin{cases}a,\;\omega\in A\\b,\;\omega\in A^c\end{cases}\quad (a\ne b)$, then $\sigma(f)=\{\Omega,\varnothing,A,A^c\}$
- **Fact**. $\{B\subset\Omega_2:f^{-1}(B)\in\mathcal{F}_1\}$ is a $\sigma$-field in $\Omega_2$, can we call it deduced $\sigma$-field

  - Still we can verify $\sigma$-field conditions
  - It is largest $\sigma$-field in $\Omega_2$ to make $f$ measurable
  - **Implication**: if $\mathcal F_2=\sigma(\mathcal A_2)$, then to check $f$ is measurable, we only need to check $\forall A\in\mathcal A_2$, we have $f^{-1}(A)\in\mathcal F_1$. The proof is:
    - Since $\{B\subset\Omega_2:f^{-1}(B)\in\mathcal{F}_1\}$ is $\sigma$-field, if we verified $f^{-1}(A)\in\mathcal F_1, \forall A\in\mathcal A_2$, then it means this $\sigma$-field contains $\mathcal A_2$
    - Also since $\{B\subset\Omega_2:f^{-1}(B)\in\mathcal{F}_1\}$ is largest $\sigma$-field in $\Omega_2$ to make $f$ measurable, then it must be the case that $\{B\subset\Omega_2:f^{-1}(B)\in\mathcal{F}_1\}\supset\mathcal F_2=\sigma(\mathcal A_2)$. 
    - That means, for all $B\in\mathcal F_2=\sigma(\mathcal A_2)$, we have $f^{-1}(B)\in\mathcal{F}_1$, which is exactly definition of measurable function

- **Special Case: Borel Measurable**. $f:(\Omega_1,\mathcal F_1)\to (\mathbb R,\mathcal B)$ is measurable if and only if for any $x\in\mathbb R$, we have $f^{-1}((-\infty,x])\in\mathcal F_1$ 

  - Think of the previous **implication**. Since $\mathcal B=\sigma(\{-\infty,x]:x\in\mathbb R\}$, the inner collection of sets is $\mathcal A_2$, and if the inverse of its elements are within $\mathcal F_1$, then its $\sigma$-algebra $\mathcal B$ could also make $f$ measurable

- **Proposition**. Let $(\Omega_1,\mathcal{F}_1),(\Omega_2,\mathcal{F}_2),(\Omega_3,\mathcal{F}_3)$ be measurable spaces. If $f_1: \Omega_1\to \Omega_2$ and $f_2: \Omega_2\to \Omega_3$, then $f_2\circ f_1: \Omega_1\to \Omega_3$ is also measurable

  - Proof: $(f_2\circ f_1)^{-1}(A)=f^{-1}(f^{-2}(A))\in\mathcal F_1$, which satisfy the definition

- **Definition**. If there is a measure $\mu_1$ on $(\Omega_1,\mathcal F_1)$, and a measurable function: $f:\Omega_1\to\Omega_2$, then $\mu_1$ **induces a measure** $\mu_2$ on $(\Omega_2,\mathcal F_2)$ such that $\mu_2(A)=\mu_1(f^{-1}(A))$. Check $\mu_2$ is a measure:

  - $\mu_2(B)\ge0$
  - $\mu_2(\varnothing)=\mu_1(f^{-1}(\varnothing))=0$
  - $\mu_2(\cup_{i=1}^\infty A_i)=\mu_1(f^{-1}(\cup_{i=1}^\infty A_i))=\mu_1(\cup_{i=1}^\infty f^{-1}(A_i))$. Since $A_1,...,$ are disjoint, then $f^{-1}(A_i),i=1,...$ are also disjoint. So $\mu_1(\cup_{i=1}^\infty f^{-1}(A_i))=\sum_{i=1}^\infty\mu_1(f^{-1}(A_i))=\sum_{i=1}^\infty\mu_2(A_i)$




### 2.2 Random Variables

- **Definition**. If $f:(\Omega,\mathcal F)\to(\mathbb R,\mathcal B)$ is measurable, then $f$ is called a real-valued (or one-dimensional) random variable, denoted by $X$; If $f:(\Omega,\mathcal F)\to(\mathbb R^d,\mathcal B)$ is measurable, then $f$ is called d-dimensional random variable, denoted by $X=(X_1,X_2,..,X_d)^T$

  - Essence: random variable is a measurable function, which needs to satisfy properties of measurable function

- **Proposition**. $X=(X_1,\ldots,X_d)^\top $ is a random vector if and only if $X_i$ is a random variable for all $1\le i\le d$. Proof:

  - Note that the proof need above **implication** that if $\mathcal F_2=\sigma(\mathcal A_2)$, then to check $f$ is measurable, we only need to check $\forall A\in\mathcal A_2$, we have $f^{-1}(A)\in\mathcal F_1$. So we only cares about $(a,b]$.

  - $\Rightarrow$: consider sets operations of $\mathcal F$ on $d$ dimensions
    $$
    X_i^{-1}((a,b])=X^{-1}(\mathbb{R}\times\cdots\times\mathbb{R}\times(a,b]\times\mathbb{R}\times\cdots\times\mathbb{R})\in\mathcal{F}
    $$

  - $\Leftarrow$:
    $$
    X^{-1}((a_1,b_1]\times\cdots\times(a_d,b_d])=[X_1^{-1}((a_1,b_2])]\cap\cdots\cap[X_d^{-1}((a_d,b_d])]\in\mathcal{F}
    $$

- **Notation**. $\sigma(X)$ is smallest $\sigma$-field to make every $x_i$ measurable (or in another words, make $X$ measurable). 

  - It could also be written as $\sigma(\sigma(X_1),...,\sigma(X_d))$
  - $d\rightarrow\infty\Rightarrow$ infinite sequence of random variables

- **Fact**. $X_1,...,X_n$ are all random variables and $f:(\mathbb R^n,\mathcal B^d)\Rightarrow (\mathbb R,\mathcal B)$ is a measurable function, then $f(X_1,...,X_n)$ is a random variable

  - Proof: Since $X=(X_1,X_2,..,X_d)^T$ is a measurable function (random variable), then $f(X_1,...,X_n)=f(X)$ is nested random variable $f\circ X$
  - Example: $X_1+X_2$. It also applies to $f$ as sort of limit. The theorem to be introduced is about this.

- **Definition**. Extended real line is $\sigma$-algebra generated by $\mathcal B\cup \{-\infty\}\cup\{-\infty\}\cup [-\infty,\infty]$ 

  - Overall, extended real line add infinity to Borel subsets

- **Definition**. If $f:\Omega\to[-\infty,\infty]$ is measurable, then it is called generalized random variable

- **Theorem**. If $X_1,..,X_n$ are random variable, then the following limits are also random variables
  $$
  \inf_{n\geq1}X_n,\quad\sup_{n\geq1}X_n,\quad\lim\sup_{n\to\infty}X_n,\quad\liminf_{n\to\infty}X_n,
  $$

  - Essentially, all above are some explicit format of $f(X_1,..,X_n)$, but they are about limit of these variables

  - Proof is through checking whether some events are in $\mathcal F$ or not. E.g., $\{\inf_{n\geq1}X_n<a\}=\{\omega:\inf_{n\geq1}X_n(\omega)<a\}$. So you can see the below proof:
    $$
    \begin{aligned}
    \{\inf_{n\geq1}X_n<a\}&=\cup_{n\geq1}\{X_n<a\}\in\mathcal{F}.\\
    \{\sup_{n\geq1}X_n>a\}&=\cap_{n\geq1}\{X_n>a\}\in\mathcal{F}.\\
    \limsup_{n\to\infty}X_n:&=\lim_{n\to\infty}\sup_{m\ge n}X_m=\inf_{n\geq1}(\sup_{m\geq n}X_m).\\
    \liminf_{n\to\infty}X_n:&=\lim_{n\to\infty}\inf_{m\ge n}X_m=\sup_{n\geq1}(\inf_{m\geq n}X_m).
    \end{aligned}
    $$

- **Definition**. Let
  $$
  \Omega_0:=\{\omega\in\Omega:\lim_{n\to\infty}X_n(\omega)\text{ exists}\}=\{\omega\in\Omega:\limsup_{n\to\infty}X_n(\omega)=\liminf_{n\to\infty}X_n(\omega)\}\in\mathcal{F}
  $$
   Then if $\mu(\Omega_0)=\mu(\Omega)$, then we say $X_n$ converges almost everywhere (a.e.). (which means we consider a general measure). If $\mu(\Omega_0)=\mu(\Omega)$, then we say $X_n$ converges almost surely (a.s.) (which means the measure is probability measure)

  - we can see the limit exist is defined as $\underset{{n\to\infty}}\limsup X_n(\omega)=\underset{{n\to\infty}}\liminf X_n(\omega)$ for almost all $\omega\in\Omega$. When saying almost all, we mean we allow for some $\omega\in\Omega$, the above equation does not hold, as long as their Lebesgue measure is zero
  - $\underset{{n\to\infty}}\liminf X_m=\sup_{n\geq1}(\inf_{m\geq n}X_m)$ could be understood like this: Let $Y_n=\inf_{m\geq n}X_m$, and then $Y_n$ is non-decreasing sequence of random variables: Think of $n_2>n_1$, then the infimum of $X_{n_1},...,X_{n_2},...$ must be less than or equal to infimum of $X_{n_2},...$ (because some r.v. are removed from the former one, thus the minimum number may be removed, making the minimum number larger). So when $n\to\infty$, $Y_n$ becomes larger and larger, which means $\underset{{n\to\infty}}\liminf X_m=\lim_{n\to\infty} Y_n=\sup_n Y_n$. 



### 2.3 Distribution

- **Definition**. Let $(\Omega,\mathcal{F},P)$ be a probability space. Let $X:\Omega\to\mathbb{R}$ be a real valued random variable. The induced measure:
  $$
  \mu(A):=P(\{w\in\Omega:X(w)\in A\})=:P(X\in A)
  $$
  is called the probability measure (or probability distribution) of $X$

- Definition. The distribution function (d.f.) of $X$ is defined to be $F:\mathbb R\to [0,1]$
  $$
  F(x)=F_X(x)=P(X\leq x).
  $$

- **Properties of d.f. and Proof**.
  $$
  \begin{array}{l}
  \mathrm{(a)~}F\text{ is non-decreasing.}\\
  \mathrm{(b)~}F\text{ is right-continuous.}\\
  \mathrm{(c)~}\lim_{x\to-\infty}F(x)=0;\lim_{x\to\infty}F(x)=1.
  \end{array}
  $$

  - (a). If $x<y$, then $F(x)=\mu(A)=P(\{\omega\in\Omega:X(\omega)\le x\}),F(x)=\mu(B)=P(\{\omega\in\Omega:X(\omega)\le y\})$. Since $A\in B$, then with measure property of monotonicity, we have $\mu(A)=F(x)<F(y)=\mu(B)$
  - (b). If $x_n\downarrow x$, then $F(x_n)=\mu(x_n)=P(\{\omega\in\Omega:X(\omega)\le x_n\})$. Note that event $\{\omega\in\Omega:X(\omega)\le x_n\}\downarrow \{\omega\in\Omega:X(\omega)\le x\}$, then $F(x_n)\downarrow F(x)$, which means right continuity
  - (c). When $x\rightarrow-\infty$, it means the event converges to $\varnothing$, while when $x\rightarrow\infty$, it means the event converges to $\Omega$, so proof done.

- **Example**. Uniform distribution on $[0,1]$:
  $$
  F(x)=\begin{cases}0,&x\leq0\\x,&0<x<1\\1,&x\geq1,\end{cases}\\
  F(b)-F(a)=b-1,0<a<b<1
  $$

- **Proposition**. If $X$ has continuous d.f. $F$, then $Y:=F(X)$ has the uniform distribution. Proof:
  $$
  P(Y\leq y)=P(F(X)\leq y)=P(X\leq F^{-1}(y))=F(F^{-1}(y))\stackrel{\text{by continuity}}{=}y.
  $$

  - Note that we impose continuity (from both sides, instead of just right continuous by definition) constraint because if not continuous, $Y$ cannot take full space of $[0,1]$. For example, below d.f. cannot make $Y$ take the value between $[\frac13,\frac23]$
    $$
    F(x)=\begin{cases}
    \frac{2x}3,\quad\;0\le x<\frac12\\
    \frac{1+2x}3,\;\;\frac12\le x\le1\end{cases}
    $$
  
  - Here $F^{-1}(\omega):=\inf\{y:F(y)\geq\omega\}=\sup\{y:F(y)<\omega\}$, just as below Theorem
  
- **Theorem**. Let $\Omega=(0,1)$, $\mathcal F=\{\text{Borel sets on }(0,1)\}$. Define $X:\Omega\to\mathbb R$ to be $X(\omega)=F^{-1}(\omega)$, where:
  $$
  F^{-1}(\omega):=\inf\{y:F(y)\geq\omega\}=\sup\{y:F(y)<\omega\}.
  $$
  Then $X$ is a random variable and the d.f. of $X$ is $F$.

  - Why $X$ is random variable: $X=F^{-1}$ monotone and thus measurable (because $\mathcal F$ is not power set, and $\sigma$-algebra on $X$ is not in specified form, then monotonicity condition could make $X$ measurable)

  - Why $F$ is d.f.: 
    $$
    \begin{align}
    P(X\le x)&=P(\{\omega:F^{-1}(\omega)\le x\})\quad\text{(by definition of $X(\omega)$)}\\
    &=P(\{\omega:\omega\le F(x)\})\quad\text{(by definition of $F^{-1}(\omega)$)}\\
    &=F(x)\quad\text{(because $P$ on $\Omega$ is Lebesgue measure)}
    \end{align}
    $$
    



### 2.4 Expectations

- **Set of discontinuity points** of distribution $F$: $\{x\in\mathbb R:\underset{y\uparrow x}{\lim}F(y)\ne F(x)\}$
  - Note that this set is countable because it could be listed as $\{a_1,a_2,...\}$
  - Let $b_i=F(a_i)-F(a_i^-)$, where $a_i$ is discontinuous point, and $F(a_i^-)$ is left limit
  - If $\sum_{i=1}^\infty b_i=1$: $F$ is discrete distribution. e.g. poisson distribution: $P(k)=\frac{e^{-\lambda}\lambda^k}{k!}$
  - If $\sum_{i=1}^\infty b_i=0$: $F$ is continuous distribution. e.g. poisson distribution: $P(k)=\frac{e^{-\lambda}\lambda^k}{k!}$
    - If $F(x)=\int^x f(y)dy,\forall x$, then $F$ is absolutely continuous with density $f$. (e.g., normal distribution)
    -  If a distribution function concentrate on a set (the set is with Lebesgue measure of zero) and the probability of each point in the set is zero. (e.g., Cantor set.)
  - Continuous, uniformly continuous, and absolutely continuous
    - Continuous: for any given $x_0$, then $\forall \epsilon>0,\exists\delta>0$ such that $\forall x:|x-x_0|<\delta\Rightarrow |f(x)-f(x_0)|<\epsilon$
    - Uniformly continuous: $\forall \epsilon>0,\exists\delta>0$ such that $\forall x_1,x_2:|x_1-x_2|<\delta\Rightarrow |f(x_1)-f(x_2)|<\epsilon$
      - Example is $f(x)=x^2$: suppose $\epsilon_0<\epsilon,\delta_0<\delta$, then to make $f(x_1+\delta_0)-f(x_1)=\epsilon_0$, $\delta_0$ needs to be smaller and smaller when $x_1$ goes to infinity. It means whenever we pick a $\delta$ such that the codomain difference is within $\epsilon$ for some domain, we need $\delta$ to be smaller to make codomain difference within $\epsilon$ when $x$ goes to much larger
      - Continuous functions can fail to be uniformly continuous if they are unbounded on a bounded domain see [wiki](https://en.wikipedia.org/wiki/Uniform_continuity)
    - Absolutely continuous: $\forall \epsilon>0,\exists\delta>0$, if any collection of disjoint sets of $(x_k,y_k)$ satisfies $\sum_{k=1}^N(y_k-x_k)<\delta $, then $\sum_{k=1}^N|f(y_k)-f(x_k)|<\varepsilon$
      - Example: $f(x)=\left\{\begin{matrix}0,&\text{if }x=0\\x\sin(1/x),&\text{if }x\neq0\end{matrix}\right.$    because the construction by [manifoldcurious](https://math.stackexchange.com/questions/406332/showing-that-fx-x-sin-1-x-is-not-absolutely-continuous-on-0-1)
      - It is essentially bounded variation, where total variation is $V_a^b(f)=\int_a^b|f^\prime(x)|dx:=\lim_{\Pi\to0}\sum_{i=2}^N|f(x_i)-f(x_{i-1})|$



#### 2.4.1 Expectations

- All below random variables are defined on $(\Omega,\mathcal F,P)$ and the expectation of general random variables are constructed in below four steps

- **Definition 1**. Given $A\in\mathcal F$, define **indicator random variable** and expectation as:
  $$
  X(\omega)=1_A(\omega)=\begin{cases}1,&\text{if }\omega\in A\\0,&\text{if }\omega\notin A.\end{cases}\quad\Rightarrow\quad E(1_A(\omega))=P(A)
  $$

- **Definition 2**. Let $A_1,...,A_n\in\mathcal F$ be disjoint and $a_1,...,a_n\in\mathbb R$. Then $\text{Let }X=\sum_{i=1}^na_i1_{A_i}$ is **simple random variable** and its expectation is defined to be:
  $$
  E(X)=\sum a_iP(A_i)
  $$

- **Fact**. If the definition 2 holds and if $\text{Let }X=\sum_{i=1}^na_i1_{A_i}=\sum_{j=1}^mb_i1_{B_i}=Y$, then $\sum_{j=1}^n a_iP(A_i)=\sum_{j=1}^n b_jP(B_j)$. Proof:

  - Note that the space formed by collection of $A_i$ and the space formed by collection of $B_i$ could be different and they could be both smaller than the full sample space. So we discuss this general case. Special case is both two collections of sets form the full sample space. However, we should note that in the general case, we must take the complement (relative to the full sample space) of the space formed by collection of $A_i$ or $B_i$
    
  - We can let:
      $$
    X-Y=\sum_{i=1}^n\sum_{j=1}^m(a_i-b_j)1_{A_i\cap B_j}+\sum_{i=1}^na_i1_{A_i\cap B_C}+\sum_{j=m}^nb_j1_{A_C\cap B_j}\\
      \text{where }A_C=(\cup_{i=1}^nA_i)^c, B_C=(\cup_{j=1}^nB_j)^c
    $$
  
  - Actually it is equivalent to the partition on the whole sample space $\Omega$: 
      $$
      X=X+0=\left(\sum_{i=1}^na_i1_{A_i}\right)+a_{n+1}1_{(\cup_{i=1}^nA_i)^c}=\sum_{i=1}^{n+1}a_i1_{A_i},\\
      Y=Y+0=\left(\sum_{j=1}^nb_j1_{B_j}\right)+b_{n+1}1_{(\cup_{j=1}^nB_j)^c}=\sum_{j=1}^{n+1}b_j1_{B_j},\\
      \text{that is to say: }a_{n+1}=b_{n+1}=0
      $$
  
  - Now we reconsider:
      $$
      \begin{align}
      E(X-Y):&=E\left(\sum_{i=1}^{n+1}\sum_{j=1}^{n+1}(a_i-b_j)1_{A_i\cap B_j}\right)\\
      &=\sum_{i=1}^{n+1}\sum_{j=1}^{n+1}(a_i-b_j)P(A_i\cap B_j)\quad\text{(by def 2)}\\
      &=\sum_{i=1}^{n+1}a_i\sum_{j=1}^{n+1}P(A_i\cap B_j)-\sum_{j=1}^{n+1}b_i\sum_{i=1}^{n+1}P(A_i\cap B_j)\quad\text{(two sums operations)}\\
      &=\sum_{i=1}^{n+1}a_iP(A_i)-\sum_{j=1}^{n+1}b_iP(B_j)\\
      &=E(X)-E(Y)=0
      \end{align}
      $$
  
  - So $E(X)=E(Y)$.
  
- **Definition 3**. **Nonnegative random variable** $X(\omega)\ge0,\forall\omega\in\Omega$, which could be constructed by simple random variable:
  $$
  B_{k,n}=\left\{x;\frac k{2^n}\leq g(x)<\frac{k+1}{2^n}\right\},\mathrm{~}k=0,1,2,\ldots4^n-1.\\
  X_n=\sum_{k=0}^{4^n-1}\frac k{2^n}\mathbb{I}_{B_{k,n}}(x)\to X
  $$
  and the expectation:
  $$
  \\E(X):=\underset{\substack{{Y:0\leq Y\leq X} \\ Y\text{ is a simple random variable}}}\sup E(Y)
  $$
  Note $E[X]$ could be $\infty$ (which means it could be defined)
  
- **Definition 4**. Define Arbitrary random variable $X$ as  $X=X^+-X^-=\max(X,0)-\max(-X,0)$
  
  - If $E(X^+)=E(X^-)=\infty$, then we say the expected value of $X$ does not exist, otherwise $E(X)=E(X^+)-E(X^-)$ (this property is from definition of expectation by Lebesgue integral)
  - If both $E(X^+),E(X^-)<\infty$, then $E(X),E(|X|)<\infty$, which means the expectation are well defined
  - Example: $X=\begin{cases}0,\;\;\omega\in A\\\infty,\;\;\omega\in A^c\end{cases}$. Then 
    - if $P(A^c)=m>0$, then $E(X)=\underset{Y:0\leq Y\leq X}\sup E(Y)=\lim_{m\to\infty} m\cdot P(A^c)=\infty$
    - if $P(A^c)=0$, then $E(X)=\underset{Y:0\leq Y\leq X}\sup E(Y)=\lim_{m\to\infty} m\cdot P(A^c)=0$
    - This implies if $X=Y$ almost surely, then $E(X)=E(Y)$

#### 2.4.2 Properties of Expectations

- **Properties**
  $$
  \begin{align}
  &\text{(a) If }X\geq Y\text{ a.s., then }E(X)\geq E(Y)\quad\text{(monotonicity)}\\
  &\text{(b) }E(aX) = aE(X)\quad\text{(linearity)}\\
  &\text{(b) }E(X+Y) = E(X)+E(Y)\quad\text{(linearity)}\\
  \end{align}
  $$
  
- Proof of $(a)$

  - Indicator random variable: let $B\subset A$, and $X=1_A(\omega),Y=1_B(\omega)$. Since $E(X)=P(A),\; E(Y)=P(B)$, given property of measures, we have $P(B)\le P(A)$, which is $E(Y)\le E(X)$
    
  - Simple random variable: Let $X=\sum_{i=1}^na_i1_{A_i},\;\text{Let }Y=\sum_{j=1}^mb_j1_{B_j}$, then since $X-Y\ge0$ a.s., then $E(X-Y)\ge0$ and according to above **Fact**, we have $E(X)\ge E(Y)$
    
  - Nonnegative random variable:
    $$
    E(X):=\underset{\substack{{Z:0\leq Z\leq X} \\ Z\text{ is simple}}}\sup E(Z),\;\;E(Y):=\underset{\substack{{Z:0\leq Z\leq Y} \\ Z\text{ is simple}}}\sup E(Z)
    $$
    

    - Since $0\le X\le Y$, then $\{E(Z):{Z:0\leq Z\leq X},Z\text{ is simple}\}\sub \{E(Z):{Z:0\leq Z\leq Y},Z\text{ is simple}\}$. 
    - Also, from simple random variable monotonicity, we have: $E(Z)\ge E(X)$ for $X\le Z\le Y$, then with monotonicity of $\sup$, we have $E(X)\le E(Y)$

  - Arbitrary random variable:
    $$
    X\ge Y \Rightarrow X^+\ge Y^+,X^-\le Y^-\\
    \Rightarrow E(X):=E(X^+)-E(X^-)\ge E(Y^+)-E(Y^-):= E(Y)  
    $$
    

- Proof of $(b)$

  - Indicator random variable: since $aX=a\cdot1_A(\omega)$, we know $E(aX)=a\cdot1\cdot P(A)+a\cdot0\cdot P(A)=a\cdot P(A)=aE(X)$

    - Note that this lecture does not define expectation with Lebesgue integral, but still relies on the property of Lebesgue integral:
    - *(i) If $X$ takes only finitely many values $y_0,y_1,y_2,\ldots,y_n$, then $\int_\Omega X(\omega)\:d\mathbb{P}(\omega)=\sum_{k=0}^ny_k\mathbb{P}\{X=y_k\}$*

  - Simple random variable: 
    $$
    \begin{align}
    E(aX)&=E\left(\sum_{i=1}^naa_i1_{A_i}\right)=\sum_{i=1}^naa_iP(A_i)\quad\text{(by def 2)}\\
    &=a\sum_{i=1}^na_iP(A_i)=aE(X) \quad\text{(still def 2)}
    \end{align}
    $$

  - Nonnegative random variable: 

    - When $a>0$: 

    $$
    \begin{align}
    E(aX)&=\underset{Y:0\leq \frac Ya\leq X,\; Y\text{ is simple}}\sup E\left(a\cdot\frac Ya\right)=\underset{Y:0\leq Y\leq X,\; Y\text{ is simple}}\sup E\left(a\cdot Y\right)\\
    &=a\cdot\underset{\substack{Y:0\leq Y\leq X \\ Y\text{ is simple}}}\sup E\left(Y\right)\quad\text{(because of the simple random variable)}
    \end{align}
    $$

    - When $a=0$, simple

    - When $a<0$:
      $$
      E(aX)\overset{DEF}=-E(-aX)=-(-a)E(X)=aE(X)
      $$

  - Arbitrary random variable

    - $a>0$: 
      $$
      \begin{align}
      E(aX)&\overset{DEF}=E[(aX)^+-(aX)^-]\\
      &=E[aX^+]-E[aX^-]\\
      &=aE[X^+]-aE[aX^-]\\
      &=aE(X)
      \end{align}
      $$

    - $a\le0$ similar

- Proof of $(c)$

  - Indicator function: $X+Y=1_A(\omega)+1_B(\omega)$, then
    $$
    E(X+Y)=2\cdot P(A\cap B)+1\cdot [P(A\cap B^c)+P(A^c\cap B)]=P(A)+P(B)=E(A)+E(B)
    $$

  - Simple random variable: proved by the **Fact** above
  
  - Nonnegative random variable: when $X,Y\ge 0$ and $X,Y$ are bounded
  
    - Since boundedness, we let $0\le X,Y\le n$. 
  
    - Also let $M\ge 2n$ be an integer and nonnegative random variable $W\le2n$. Then define:
      $$
      W_M^{(l)}:=\lfloor2^MW\rfloor/2^M,\quad W_M^{(u)}:=W_M^{(l)}+\frac1{2^M},
      $$
  
    - Also we can show:
      $$
      \lfloor2^MW\rfloor\le2^MW<\lfloor2^MW\rfloor+1\quad\text{(by definition of floor and ceil)}\\
      \Rightarrow \frac{\lfloor2^MW\rfloor}{2^M}\le W<\frac{\lfloor2^MW\rfloor}{2^M}+\frac1{2^M}\\
      \Rightarrow W_M^{(l)}\le W<W_M^{(l)}+\frac1{2^M}
      $$
  
    - So actually the relation $\frac{\lfloor2^MW\rfloor}{2^M}\le W<\frac{\lfloor2^MW\rfloor}{2^M}+\frac1{2^M}$ means we divide the $W$ sample space $[0,2n]$ into subintervals of length $\frac1{2^M}$, and there are $2^M\cdot 2n$ such subintervals. Meanwhile, the relation told us that we can put each realization of $W$ into certain subinterval, and the subinterval grows with $M$ and eventually converges to all points in the sample space. Also, $W_M^{(l)}$ could be regarded as simple random variable, as is shown in the subinterval process
  
    - Go back to $X,Y$. We let them in $[0,n]$ to ensure $X+Y\in[0,2n]$, as is $W$. Thus by upper bound:
      $$
      \begin{aligned}
      E(X+Y)&< E[(X+Y)_M^{(u)}]&(\text{the symbol '$<$' could be '$=$' only if $M\to\infty$})\\
      &\leq E[X_M^{(u)}+Y_M^{(u)}]&(\text{from the sub-additivity of the operator }(\cdot)_M^{(u)})\\
      &=E[X_M^{(u)}]+E[Y_M^{(u)}]&(\text{from the linearity for simple random variables})\\
      &\leq E(X)+\frac1{2^M}+E(Y)+\frac1{2^M}
      \end{aligned}
      $$
      So if $M\to\infty$, we can have $E(X+Y)= E[(X+Y)_M^{(u)}]$ and all the rest $\le$ could hold
  
    - Similarly for lower bound, we have $E(X+Y)\ge E(X)+E(Y)$ when $M\to\infty$. Thus $E(X)+E(Y)=E(X+Y)$
      $$
      \begin{aligned}
      E(X+Y)&\ge E[(X+Y)_M^{(l)}]\\
      &\geq E[X_M^{(l)}+Y_M^{(l)}]&(\text{from the sub-additivity of the operator }(\cdot)_M^{(u)})\\
      &=E[X_M^{(u)}]+E[Y_M^{(u)}]&(\text{from the linearity for simple random variables})\\
      &\geq E(X)+\frac1{2^M}+E(Y)+\frac1{2^M}
      \end{aligned}
      $$
  
  - Nonnegative random variable: when $X,Y\ge 0$ and $X,Y$ could be **unbounded**, i.e., $X,Y$ could be infinity
  
    - First, we can check: 
      $$
      (X\wedge n)+(Y\wedge n)\leq(X+Y)\wedge2n\leq(X\wedge2n)+(Y\wedge2n).\\\Downarrow\\
      E[(X\wedge n)+(Y\wedge n)]\leq E[(X+Y)\wedge2n]\leq E[(X\wedge2n)+(Y\wedge2n)]\\\Downarrow\\
      E[(X\wedge n)]+E[(Y\wedge n)]\leq E[(X+Y)\wedge2n]\leq E[(X\wedge2n)]+E[(Y\wedge2n)]\\
      \text{(because the linearity proved above under boundedness)}
      $$
  
    - Given nonnegative random variable $W$, we have $E(W\wedge n)\uparrow E(W)$ as $n\uparrow\infty$. Thus when $n\to\infty$, then above inequality becomes:
      $$
      E[(X)]+E[(Y)]\leq E[(X+Y)]\leq E[(X)]+E[(Y)]\\\Downarrow\\
      E[(X)]+E[(Y)]= E[(X+Y)]
      $$
  
  - Arbitrary random variable: when $E(|X|),E(|Y|)<\infty$. So:
    $$
    X+Y = (X^+-X^-)+(Y^+-Y^-)\\
    X+Y = (X+Y)^+-(X+Y)^-
    $$
  
    - Note that the first one equation above is expressed by extend $X,Y$ as two random variables, while the second equation is by extending $X+Y$ as a whole random variable. Then we have:
      $$
      (X^+-X^-)+(Y^+-Y^-)=
      X+Y = (X+Y)^+-(X+Y)^-
      $$
      
  
       and **it is wrong that** $X^++Y^+=(X+Y)^+,X^-+Y^-=(X+Y)^-$
  
    - Take expectations:
      $$
      E[(X+Y)^++(X^-+Y^-)]=E[(X+Y)^-+(X^++Y^+)];\\\Downarrow\text{(linearity of nonegative r.v.)}\\
      E[(X+Y)^+]+E[(X^-+Y^-)]=E[(X+Y)^-]+E[(X^++Y^+)]
      $$
  
    - Therefore:
      $$
      \begin{aligned}E(X+Y)=&E(X+Y)^+-E(X+Y)^-&\text{(Definition 4 of the expectation)}\\=&E(X^++Y^+)-E(X^-+Y^-)&\text{(From (3.2))}\\=&E(X^+)+E(Y^+)-E(X^-)-E(Y^-)\\&\text{(From the linearity of }E\text{ for nonnegative random variables)}\\=&E(X)+E(Y).&\text{(Definition 4 of the expectation)}\end{aligned}
      $$
  
- **Theorem**. Monotone Convergence Theorem (MCT): Let $\{X_{n}\:\geq\:0,n\:=\:1,2,\ldots\}$ be a sequence of nonnegative random variables. If $X_n\uparrow X$, then $E(X_n)\uparrow E(X)$. Proof:

  - $X_n\uparrow X$ means $0\le X_1\le X_2\le\cdots$ and $\lim_{n\to\infty}X_n=\sup_{n\ge1} X_n=X$. 

  - $E(X_n)\uparrow E(X)$ means $0\le E(X_1)\le E(X_2)\le\cdots$ and $\lim_{n\to\infty}E(X_n)=\sup_{n\ge1}E(X_n)=E(X)$. 

  - We do not impose further condition on $X$ to make sure it could be either bounded or not bounded. Then $E(X)$ could either converge to $\infty$ (essentially this is divergence) or converge to a finite number. We let $\lim_{n\to\infty}E(X_n)=a$, where a could either be infinity or finite number

  - When $a=\infty$: because $E(X)\ge E(X_n)\forall n$ (since $X=\sup_n X_n, X\ge X_n>0$ ), then when $a=\infty$, it means $E(X)\ge a=\infty$, which implies $E(X)=\infty$

  - When $a<\infty$: we still have $E(X)\ge a$. Then to show $E(X)=a$, we only need to show $E(X)\le a$. And by definition 3 of nonnegative r.v., we have:
    $$
    E(X):=\sup_{\begin{array}{c}Y:0\leq Y\leq X\\Y\text{is a simple random variable}\end{array}}E(Y).
    $$
    To to show $E(X)\le a$, we only need to show for all $\epsilon>0$ and all $Y:0\le Y\le X$, we have $E(Y)\le a+\epsilon$.

    - fix $\epsilon$ and given one $Y=\sum_{j=1}^mb_j1_{B_j}$ ($Y$ could be any one of the collection specified above), where $B_j$ are pairwise disjoint

    - Also define $Y_\varepsilon=\sum_{j=1}^m(b_j-\frac\varepsilon2)1_{B_j}.$ Then we have:
      $$
      \begin{aligned}
      E(X_n)=&E[X_n1(X_n\geq Y_\varepsilon)]+E[X_n1(X_n<Y_\varepsilon)]&\text{(Bayesian)}\\
      \geq&E[Y_\varepsilon1(X_n\geq Y_\varepsilon)]+E[X_n1(X_n<Y_\varepsilon)]&\text{(think of the indicator r.v. when it is true)}\\
      \geq&E(Y_\varepsilon)-E[M1(X_n<Y_\varepsilon)]&\text{(For a sufficiently large constant }M)\\=&E(Y_\varepsilon)-MP(X_n<Y_\varepsilon)\\\geq&E(Y_\varepsilon)-\frac\varepsilon2,&\text{(For sufficiently large }n)\end{aligned}
      $$
      the last step is because $\{X_n<Y_\varepsilon\}\to\varnothing$ when $n\to\infty$ (think of supremum), and thus $MP(X_n<Y_\varepsilon)\to0$, which is larger than $-\frac\varepsilon2$. 

    - Thus we have:
      $$
      E(Y)\overset{\text{Definition of }Y_\varepsilon}{\operatorname*{\leq}}E(Y_\varepsilon)+\frac\varepsilon2\overset{\text{Above inequality}}{\operatorname*{\leq}}E(X_n)+\varepsilon\leq a+\varepsilon.
      $$

- **Lemma**. Fatou’s Lemma. If $X_n\ge0,\forall n$, then: $\underset{n\to\infty}\liminf E[X_n]\geq E[\underset{n\to\infty}\liminf X_n]$. Proof:

  - When Let $Y_n=\inf_{m\ge n}X_m$, then
    $$
    \underset{n\to\infty}\liminf X_n:=\underset{n\to\infty}\lim\inf_{m\ge n}X_m=\underset{n\to\infty}\lim Y_n
    $$

  - Note that we also have:
    $$
    \underset{n\to\infty}\liminf X_n:=\underset{n\to\infty}\lim\inf_{m\ge n}X_m:=\sup_{n\ge1}\inf_{m\ge n}X_m
    $$
    
  - Since $Y_n\ge 0$ and is non-decreasing, and therefore $\lim Y_n=\sup_{n\ge 1}Y_n\Rightarrow Y_n\uparrow(\sup_{n\ge 1}Y_n)$. By MCT:
    $$
    E(Y_n)\uparrow E(\sup_{n\ge 1}Y_n)\\
    Y_n:=\inf_{m\ge n}X_m;\;\;\;\sup_{n\ge 1}Y_n:=\underset{n\to\infty}\liminf X_n\\\Downarrow\\
    \lim_{n\to\infty}E(\inf_{m\ge n}X_m)=E(\underset{n\to\infty}\liminf X_n)
    $$
  
  - Also, it is easy to see that $\inf_{m\ge n}X_m\le X_{k},\forall k\ge n$, which means $E(\inf_{m\ge n}X_m)\le E(X_k),\forall k\ge n$. Thus we can have $E(\inf_{m\ge n}X_m)\le \inf_{k\ge n}E(X_k)$. Change the script $k$, we have: $E(\inf_{m\ge n}X_m)\le \inf_{m\ge n}E(X_m)$
  
  - Therefore we have:
    $$
    E(\underset{n\to\infty}\liminf X_n)=\lim_{n\to\infty}E(\inf_{m\ge n}X_m)\le\lim_{n\to\infty}\inf_{m\ge n}E(X_m):=\underset{n\to\infty}\liminf E[X_n]
    $$
  
- **Theorem**. Dominated Convergence Theorem (DCT): If $X_n\to X$ a.s., and $|X_n|\le Y$ for some $Y$ with $E(Y)<\infty$. Then $E(X_n)\to E(X)$. Proof:

  - Since $|X_n|\le Y$, then $X_n+Y\ge 0$. By Fatou's lemma:
    $$
    \underset{n\to\infty}\liminf E[X_n+Y]\geq E[\underset{n\to\infty}\liminf (X_n+Y)]
    $$

  - Also, since $X_n\overset{a.s.}\to X$, then it must be the case that $(X_n+Y)\overset{a.s.}\to (X+Y)$. Also, almost surely convergence means $P\{\omega:\underset{n\to\infty}\liminf (X_n(\omega)+Y(\omega))=\underset{n\to\infty}\limsup (X_n(\omega)+Y(\omega)):=X(\omega)+Y(\omega)\}=1$. Thus the RHS becomes:
    $$
    E[\underset{n\to\infty}\liminf (X_n+Y)]=E(X+Y)=E(X)+E(Y)
    $$

  - Now the LHS:
    $$
    \underset{n\to\infty}\liminf E[X_n+Y]=\underset{n\to\infty}\liminf [E(X_n)+E(Y)]=\underset{n\to\infty}\liminf E(X_n)\quad +\quad E(Y)
    $$

  - LHS=RHS gives:
    $$
    \underset{n\to\infty}\liminf E(X_n)\ge E(X)
    $$

  - Similarly, we have:
    $$
    \begin{align}
    \lim\sup_{n\to\infty}E(X_n-Y) & = -\lim\inf_{n\to\infty}E(-X_n+Y)&{(\sup_n Z_n=-\inf_n(-Z_n))}\\
    &\leq-E[\lim\inf_{n\to\infty}(-X_n+Y)] &\text{(Fatou's lemma)}\\
    & = -E[-X+Y]&\text{(property of }\overset{a.s.}\to)\\
    &\Downarrow\\
    \lim\sup_{n\to\infty}E(X_n)-E(Y)&\le E(X)-E(Y)
    \end{align}
    $$

  - So $\underset{n\to\infty}\liminf E(X_n)\ge E(X)\ge \lim\sup_{n\to\infty}E(X_n)$. Since $\lim\sup_{n\to\infty}E(X_n)\ge \lim\inf_{n\to\infty}E(X_n)$, then it means:
    $$
    \underset{n\to\infty}\liminf E(X_n)= E(X)= \lim\sup_{n\to\infty}E(X_n)
    $$

​		QED.
- **Theorem**. Bounded Convergence Theorem (BCT): If $X_n\to X$ a.s., and $|X_n|\le M$ for all $n$, where $M$ is a constant. Then the limit r.v. $X$ is integrable ($E|X|<\infty$) and that $E(X_n)\to E(X)$. 
  - Note both BCT and DCT are used to investigate convergence of expectation, but the difference is: DCT requires domination by integrable function, while BCT requires uniform bounded by a constant 

#### 2.4.3 Inequalities

- Why introduce these inequalities: to prove asymptotics under tricks like $\epsilon$-$\delta$ language in the form of:

  - $a-\epsilon_n\le E(X_n)\le a+\epsilon_n,\forall\epsilon_n>0$
  - $c_1n\le E(X_n)\le c_2n$, for some $c_1,c_2$

- **Definition**. Let $(\Omega,\mathcal F,\mu)$ be the probability space. Then the **$L_p$ space** is defined as:
  $$
  L_p(\Omega,\mathcal F,\mu)=\{X:\Omega\to\mathbb R,E|X|^p<\infty\}
  $$

  - So essentially, it is all random variables defined on the same probability space that makes the expectation finite
  - If $p\ge 1$, then $\|X\|_p:=(E|X|^p)^{1/p}$ is a norm on $L_p(\Omega,\mathcal F,\mu)$. Some resources just define $L_p$ space when $p\ge 1$, like [here](https://assets.press.princeton.edu/chapters/s9627.pdf) 
  - Properties of norm
    - $\|X\|_p=0\Leftrightarrow X=0\;a.s.$
    - $\|aX\|_p=a\|X\|_p,\forall x\in\mathbb R$
    - $\|X+Y\|_p\le\|X\|_p+\|Y\|_p$ (Minkowski's Inequality, shown below)
  - $L_p(\Omega,\mathcal F,\mu)$ is complete, which means every Cauchy sequence of random variables in $L_p(\Omega,\mathcal F,\mu)$ converges to a limit random variable, and this limit random variable is also in $L_p(\Omega,\mathcal F,\mu)$. 
    - See more details in this [pdf](https://web.math.princeton.edu/~rvan/acm217/handout040507.pdf), definition 1 and proposition 2. 
    - This is meaningful because we can show existence of a sequence of random variables if the sequence is Cauchy sequence of random variable
  
- **Jensen's Inequality**: If $X$ is a random variable, $\psi$ is a convex function, $E|X|,E|\psi(X)|<\infty$, then:
  $$
  E[\varphi(X)]\geq\varphi[E(X)]
  $$

  - e.g. $E(|X|)\geq|E(X)|,\quad E(X^2)\geq[E(X)]^2,\quad E(e^X)\geq e^{E(X)}$

  - counter example: $E(\log x)\leq\log(E(X))$

  - Proof:

    - Let $c=E(X)$, and find $a,b$ such that $a+bc=\varphi(c)$ and $\varphi(x)\ge a+bx,\forall x$. So we can also see it as $\varphi(X)\ge a+bX$ in almost surely way

    - Since $E(\varphi(X))\ge E(a+bX)=a+bE(X)=\varphi(c)=\varphi(E(X))$

    - Plot:

      <img src="convex.png" style="zoom: 67%;" />

- **Holder’s Inequality**: If $p,q\ge 1$ and $\frac1p+\frac1q=1$, then $E|XY|\leq\|X\|_p\|Y\|_q,$ where $\|X\|_p:=(E|X|^p)^{1/p}$ 

  - Special case
    - When $p=\infty$: $\begin{aligned}\|X\|_\infty:=\inf\{a:P(|X|>a)=0\}.\end{aligned}$
    - When $p=q=2$:  Cauchy-Schwarz inequality, $E|XY|\leq \sqrt{E(X^2)E(Y^2)}$. 

  - Young's inequality for products: $xy\le\frac{x^p}p+\frac{y^q}q,\forall x,y\ge 0$ if $\frac1p+\frac1q=1$ and $p,q>1$

    - Proof of this inequality is equivalent to prove $\log\left((x^p)^{1/p}\cdot(y^q)^{1/q}\right)\le \log\left(\frac{x^p}p+\frac{y^q}q\right)$, so we need to prove:
    $$
      \frac{1}{p}\log x^{p}+\frac{1}{q}\log y^{q}\leq\log(\frac{x^{p}}{p}+\frac{y^{p}}{q})
    $$
    
    - Suppose a binary random variable $W$ take values and probability in the way of:
    $$
      P(W=x^p)=\frac1p,\quad P(W=y^q)=\frac1q,\quad\frac1p+\frac1q:=1
    $$
    
    - So we can treat LHS=$E(\log(W))$ and RHS=$\log(E(W))$ (by property of expectation on finite sets of realizations). So by Jensen's inequality (the counter example in fact), we have $LHS\le RHS$. Proof done.

  - Proof:

    - If $\|X\|_p,\|Y\|_q<\infty$ and either $\|X\|_p$ or $\|Y\|_q$ is zero (or both). Let's say $\|X\|_p=0$, then $X=0$ almost surely (i.e. $P(|X|>\epsilon)=0,\forall\epsilon>0$). Then $XY=0$ almost surely ($P(|XY|>\epsilon^\prime)=0,\forall\epsilon^\prime>0$, where $\epsilon^\prime=\epsilon\cdot|Y|$), which means in LHS=RHS in above inequality. It is the same for $\|Y\|=0$ and $|YX|=0$ almost surely.

    - If $\|X\|_p,\|Y\|_p>0$ and either $\|X\|_p=\infty$ or $\|Y\|_p=\infty$ (or both), then RHS is always $\infty$, so LHS is always less than or equal to $\infty$

    - We can check when either one of $p,q$ is zero and the other is infinity could make inequality holds, see [wiki](https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality). Then we only need to verify when $p,q>1$ the inequality still holds.

    - Then we assume $\|X\|_p=\|Y\|_p=1$, otherwise we can divide both sides of the inequality by $\|X\|_p\|Y\|_p$, then we have $\|\frac X{\|X\|_p}\|=\|\frac Y{\|Y\|_p}\|=1$ and thus we only need to prove $E(|XY|)\le 1$

    - By Young's inequality, we let $x=|X(\omega)|,y=|Y(\omega)|$, then:
      $$
      |X(\omega)Y(\omega)|\le\frac{|X(\omega)|^p}p+\frac{|Y(\omega)|^q}q
      $$
  
    - Integrating (in Lebesgue way) both sides, then:
      $$
      E|XY|\le\frac{E|X|^p}p+\frac{E|Y|^q}q=\frac{\|X\|^p_p}p+\frac{\|Y\|^q_q}q=\frac1p+\frac1q=1
      $$
      proof done.
  
- **Minkowski’s Inequality**: for $p\ge1$, we have $\|X+Y\|_p\leq\|X\|_p+\|Y\|_p.$ Proof:

  - Let $q$ be such that $\frac1p+\frac1q=1$. Then:
    $$
    \begin{align}
    (E|X+Y|^p)^{1/p}&=\left[E(|X+Y|^{p-1}\cdot|X+Y|)\right]^{1/p}\\
    &=\left(\int_\Omega|X(\omega)+Y(\omega)|\cdot|X(\omega)+Y(\omega)|^{p-1}d\mu\right)^{1/p}\\
    &\le\left(\int_\Omega(|X(\omega)|+|Y(\omega)|)\cdot|X(\omega)+Y(\omega)|^{p-1}d\mu\right)^{1/p}\quad\text{(inequality of abs)}\\
    &=\left[E(|X|\cdot|X+Y|^{p-1})+E(|Y|\cdot|X+Y|^{p-1})\right]^{1/p}\\
    &\overset{\mathrm{Hölder}}{\operatorname*{\leq}}\left[(E|X|^p)^{1/p}(E|X+Y|^{(p-1)q})^{1/q}+(E|Y|^p)^{1/p}(E|X+Y|^{(p-1)q})^{1/q}\right]^{1/p}\\
    &=(\|X\|_p+\|Y\|_p)^{1/p}(E|X+Y|^{(p-1)q})^{\frac1{pq}}
    \end{align}
    $$

  - $\frac1p+\frac1q=1$ implies $\frac{pq}{p+q}=1\Rightarrow(p-1)q=pq-q=p+q-q=p$, so we have:
    $$
    \begin{align}
    (E|X+Y|^p)^{1/p}&\le(\|X\|_p+\|Y\|_p)^{1/p}(E|X+Y|^{(p-1)q})^{\frac1{pq}}\\
    &=(\|X\|_p+\|Y\|_p)^{1/p}(E|X+Y|^p)^{\frac1{pq}}\\
    &\Downarrow\\
    E|X+Y|^p&\le(\|X\|_p+\|Y\|_p)(E|X+Y|^p)^{1/q}\\
    &\Downarrow\\
    E|X+Y|^p\cdot\frac{(E|X+Y|^p)^{1/p}}{E|X+Y|^p}&\le(\|X\|_p+\|Y\|_p)(E|X+Y|^p)^{1/q}\cdot\frac{(E|X+Y|^p)^{1/p}}{E|X+Y|^p}\\
    &\Downarrow\\
    (E|X+Y|^p)^{1/p}&\le\|X\|_p+\|Y\|_p
    \end{align}
    $$

  - Proof done. Note that to make the proof more complete, we need to show $\|X+Y\|_p$ if finite if $\|X\|_p+\|Y\|_p$ is finite. See details in [wiki](https://en.wikipedia.org/wiki/Minkowski_inequality)

- **Markov's Inequality**: If $X$ is a random variable, then for any $a>0$, we have:
  $$
  P(|X|\ge a)\le \frac{E{|X|}}a.
  $$
  Proof:
  $$
  P(|X|\ge a)=E(1_{\{|X|\ge a\}})\le E\left(1_{\{\frac{|X|}a\ge a\}}\right)\le\frac{E|X|}a
  $$

- **Chebyshev's Inequality**: 
  $$
  P(|X-E(X)|\ge a)\le \frac{Var(X)}{a^2}
  $$
  Proof done just by replacing $P(|X|\ge a)$ by $P[|X-E(X)]^2\ge a^2)$ in Markov's Inequalityfdc

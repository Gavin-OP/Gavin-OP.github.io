# 1. Introduction

## 1.1 Purpose of statistics

- What is the purpose of statistics? 
  - Want to know something about the population.
  - We have: 
    - a distribution (or population) $p(x|Θ)$ ($Θ$ may be one parameter or a parameter vector) 
    - a set of independent and identically distributed ($i.i.d.$) samples $X = {X_1, X_2, . . . , X_n}$ from $F$. 
  - The **probability** is that given the population $p(x|Θ)$, we investigate the properties of $X$. 
  - The **statistics** is that given the samples $X$, we estimate the population $p(x|Θ)$ or equivalently the unknown $Θ$.
  - In practice, the population is always unknown, so we <u>accumulate samples to obtain knowledge about the population</u> (Statistics). 
  - In the meanwhile, the procedure to gain population knowledge are largely based on the probability theory (e.g. Method of Moments, Central Limit Theory) and optimization techniques (e.g. Maximum Likelihood Estimator).

## 1.2 Two Schools of Statiticians 

### 1.2.1. Frequentist

- Parameters are **fixed**; data/samples are random
- Main Method: Find MLE (Maximum Likelihood Estimator) -- Optimization Problem
  - Find $\hat Θ$ such that $L(Θ|x_1,...,x_n)$ is maximized -- $\hat Θ$ = $argmax L(Θ|x_1,...,x_n)$
  - Log-likelihood function: $l(Θ|x_1,...,x_n) = logL(Θ|x_1,...,x_n)$

### 1.2.2. Bayesian

- Parameters are **random**; data/samples are random

- Main method -- sample from a target distribution: 

  - have some prior belief about the parameters -- prior distribution π(Θ) 
  - Observe data -- likelihood $L(Θ|x_1,...,x_n)$ 
  - Update your belief about the parameters after observing the data

- Bayes' Theorem: $P(A|B)=P(A,B)P(B)=P(B|A)⋅P(A)P(B)$ 

  ​				$p(\Theta | x) = \frac{p(x | \Theta) \pi(\Theta)}{p(x)} \\ = \frac{p(x | \Theta) \pi(\Theta)}{\int p(x, \Theta) d\Theta} \\ = \frac{p(x | \Theta) \pi(\Theta)}{\int p(x | \tau) p(\tau) d\tau} \\ = \text{const} \times p(x | \Theta) \pi(\Theta) \\ \propto p(\Theta) \pi(\Theta)$ 

  

- **Theorem 1.1**: If $f(x) = c_0ker(x)$ is a density function, and $h(x) = d_0ker(x)$ is also a density function, then $c_0 = d_0$, $f(x) = h(x)$ for $∀x$.

  - Proof: On the one hand, $f(x)$ is a $pdf$ $⇒$ $\int f(x)dx = 1$ $⇒$ $c_0 \int ker(x)dx = 1$ $⇒$ $c_0 = \frac{1}{\int ker(x)dx}$ . On the other hand, $\int h(x)dx = 1 ⇒ d_0 \int ker(x)dx = 1 ⇒ d_0 = \frac{1} {\int ker(x)dx}$ . Therefore, $c_0 = d_0$. 

- If direct maximization of $p(Θ|x_1, ..., x_n)$ is difficult, we can approximate $p(Θ|x_1, ..., x_n)$ via sampling. That is, we can draw sample from $p(Θ|x_1, ..., x_n)$.